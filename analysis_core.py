import os
import pandas as pd
import json
import re
import requests 
from datetime import datetime, timedelta
from collections import Counter 
import gc 
import matplotlib.pyplot as plt 
from matplotlib import font_manager, rc
import matplotlib
import requests.utils
import threading
import time
import markdown
import uuid  # ğŸ‘ˆ [í•µì‹¬] ì‚¬ìš©ì ê°„ íŒŒì¼ ë®ì–´ì“°ê¸° ë°©ì§€
from dotenv import load_dotenv
load_dotenv()

# Flask ì—°ë™ ë° ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•´ í•„ìˆ˜ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import io 
import numpy as np 
import shutil # ğŸ‘ˆ ìºì‹œ ì‚­ì œìš©

# íƒœê·¸ í´ë¼ìš°ë“œ & ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
from wordcloud import WordCloud 
import seaborn as sns

# ==================================================================
# [1] í°íŠ¸ ìºì‹œ ì‚­ì œ ë° ê°•ì œ ì„¤ì •
# ==================================================================
def set_custom_font():
    try:
        cache_dir = matplotlib.get_cachedir()
        if os.path.exists(cache_dir):
            shutil.rmtree(cache_dir)
    except Exception:
        pass

    current_dir = os.path.dirname(os.path.abspath(__file__))
    font_path = os.path.join(current_dir, 'static', 'NanumGothic.ttf')
    global FONT_PATH 

    if os.path.exists(font_path):
        try:
            font_manager.fontManager.addfont(font_path)
            font_prop = font_manager.FontProperties(fname=font_path)
            font_name = font_prop.get_name()
            plt.rcParams['font.family'] = font_name
            rc('font', family=font_name)
            FONT_PATH = font_path
        except Exception as e:
            print(f"-> [ì—ëŸ¬] í°íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
            FONT_PATH = None
    else:
        FONT_PATH = None

set_custom_font()
plt.rcParams['axes.unicode_minus'] = False

# ==================================================================
# [2] ì‹œê°í™” ì €ì¥ í•¨ìˆ˜ (íŒŒì¼ëª… ì¤‘ë³µ ë°©ì§€ & ìºì‹œ ë°©ì§€)
# ==================================================================
def save_and_get_url(plot_func, filename, static_folder, unique_id): # ğŸ‘ˆ unique_id ì¶”ê°€
    """ Matplotlib ê·¸ë˜í”„ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  URLì„ ë°˜í™˜í•©ë‹ˆë‹¤. """
    if not static_folder:
        return None

    try:
        img_save_path = os.path.join(static_folder, 'img')
        if not os.path.exists(img_save_path):
            os.makedirs(img_save_path)

        plot_object = plot_func()
        if plot_object is None: return None

        # íŒŒì¼ëª… ì•ì— ê³ ìœ  IDë¥¼ ë¶™ì—¬ì„œ ê²¹ì¹˜ì§€ ì•Šê²Œ í•¨
        final_filename = f"{unique_id}_{filename}"
        filepath = os.path.join(img_save_path, final_filename)

        if os.path.exists(filepath):
            os.remove(filepath)

        plot_object.savefig(filepath, dpi=100)
        plt.close('all')
        
        # URL ë’¤ì— íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë¶™ì—¬ ë¸Œë¼ìš°ì € ìºì‹œ ë°©ì§€
        timestamp = int(time.time())
        return f"/static/img/{final_filename}?v={timestamp}"

    except Exception as e:
        print(f"-> âŒ ì‹œê°í™” ì €ì¥ ì˜¤ë¥˜ ({filename}): {e}")
        plt.close('all')
        return None

# ----------------------------------------------------
# --- ì„¤ì • (Configuration) ---
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
MAX_RESULTS_PER_API = 1000 

# --- ê°ì„± ì‚¬ì „ (ê¸°ì¡´ ìœ ì§€) ---
POSITIVE_WORDS = [
    'ì¢‹ì•„ìš”', 'ìµœê³ ', 'ë§Œì¡±', 'ì¶”ì²œ', 'ê°•ë ¥ì¶”ì²œ', 'ëŒ€ë°•', 'ì˜ˆì˜', 'ì˜ˆì˜ë‹¤', 'í¸ì•ˆ', 'í¸ì•ˆí•¨',
    'í–‰ë³µ', 'ê°ì‚¬', 'ê¸°ì¨', 'í›Œë¥­', 'ì‚¬ë‘', 'ì¬ë¯¸', 'ì¦ê±°ì›€', 'ì„±ê³µ', 'í•©ê²©', 'ì„ ë¬¼',
    'ë”°ëœ»', 'ë°ì€', 'ì™„ë²½', 'ì¸ìƒí…œ', 'ê°€ì„±ë¹„', 'ì°©í•œ', 'ë†€ë', 'ê¸°ëŒ€', 'ë³´ëŒ', 'ê¹¨ë—',
    'ì‹±ê·¸ëŸ½', 'í”„ë¦¬ë¯¸ì—„', 'ì¹œì ˆ', 'ì‹ ë¢°', 'í¸ë¦¬', 'ì•ˆì •ì„±', 'í’ë¶€', 'íš¨ìœ¨ì ', 'ì„¸ë ¨', 'í”„ë Œë“¤ë¦¬',
    'ë¯¿ìŒ', 'ê°€ì¹˜', 'ë§Œì¡±ê°', 'ì¶”ì²œí•©ë‹ˆë‹¤', 'ì§±', 'ìµœê³ ì˜ˆìš”', 'ì¢‹ë„¤ìš”', 'ê°ë™', 'ì¬êµ¬ë§¤', 'ì¬êµ¬ë§¤ì˜ì‚¬',
    'ë§Œì¡±ìŠ¤ëŸ¬ì›€', 'ê³ ê¸‰', 'í€„ë¦¬í‹°', 'ë›°ì–´ë‚¨', 'ë§Œì¡±ìŠ¤ëŸ½ë‹¤', 'í›Œë¥­í•´ìš”', 'ë¯¿ì„ë§Œ', 'í¸ì•ˆí•´ìš”', 'ì‚¬ë‘ìŠ¤ëŸ¬', 'ì‚¬ë‘ìŠ¤ëŸ¬ì›€',
    'í¸ë¦¬í•¨', 'ì•ˆì •ì ', 'í’ì„±', 'ë§Œì¡±ë„', 'íš¨ìœ¨ì„±', 'ì¸ê¸°', 'ìœ ìš©', 'ì‹¤ìš©ì ', 'ê°•ì¶”', 'ê°•ì¶”í•©ë‹ˆë‹¤',
    'ì¶”ì²œí•´ìš”', 'ê°ì‚¬í•´ìš”', 'ê¸°ë»ìš”', 'ë§Œì¡±í–ˆë‹¤', 'ì¢‹ìŠµë‹ˆë‹¤', 'ìµœê³ ì„', 'í¸ì•ˆí•¨ì´', 'ì¹œì ˆí•´ìš”', 'ê°€ì„±ë¹„ì¢‹', 'ê²½ì œì ',
    'ë§Œì¡±ë„ê°€', 'ë§Œì¡±í–ˆë˜', 'ê¹”ë”', 'ê¹”ë”í•¨', 'ì‹ ì†', 'ì‹ ì†í•¨', 'ì •í™•', 'ì •í™•í•¨', 'í¸ì•ˆí•œ', 'í¬ê·¼',
    'ë‹¬ì½¤', 'ì‹œì›', 'í’ë¯¸', 'í™œë ¥', 'ì•ˆì •ê°', 'ë§Œì¡±ìŠ¤ëŸ¬ìš´', 'ë†€ëë‹¤', 'ê¸°ëŒ€ì´ìƒ', 'ì¶”ì²œë°›ìŒ', 'ê¸°ëŒ€ë§Œí¼'
]

NEGATIVE_WORDS = [
    'ë³„ë¡œ', 'ì‹¤ë§', 'ì•„ì‰¬ì›€', 'ë‚˜ì˜', 'ë¶ˆë§Œ', 'ë¶ˆí¸', 'ìµœì•…', 'í˜ë“¤', 'ê±±ì •', 'ë¬¸ì œ',
    'ì–´ë ¤ì›€', 'ë¶€ì¡±', 'ì‹¤íŒ¨', 'ë‚­ë¹„', 'ì“°ë ˆê¸°', 'ë¹„ì‹¸', 'ë–¨ì–´ì§', 'ì‹¤ë§ìŠ¤ëŸ½', 'ì§€ë£¨', 'í›„íšŒ',
    'ì´ìƒ', 'ì˜¤ë¥˜', 'ì§œì¦', 'ê³ í†µ', 'ë…¼ë€', 'ë¶€ì •ì ', 'ì•½ì ', 'ê²°í•¨', 'ë³µì¡', 'ë¶ˆí•„ìš”',
    'ë¬´ì„±ì˜', 'ë¶ˆì¹œì ˆ', 'ì§€ì €ë¶„', 'ê³¼ëŒ€ê´‘ê³ ', 'ì˜¤ë˜ê±¸ë¦¼', 'í—ˆì ‘', 'ë²ˆê±°ë¡œì›€', 'ë¶€ì •í™•', 'ë¹„ì¶”ì²œ', 'ë¶ˆì‹ ',
    'ë¶ˆë§Œì¡±', 'ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ì›€', 'ë¶ˆì¾Œ', 'ë¶ˆë§Œì¡±í•˜ë‹¤', 'ë¶ˆë§Œì¡±ìŠ¤ëŸ½ë‹¤', 'ë¶ˆì•ˆì •', 'ì§ˆë‚®', 'ì €ê¸‰', 'í˜•í¸ì—†', 'ì—‰ë§',
    'ë§í•¨', 'ì‹¤íŒ¨ì‘', 'ë¶ˆí¸í•¨', 'ì„±ê°€ì‹¬', 'ë²ˆê±°ë¡œì›Œìš”', 'ëª»í•¨', 'ë¶€ì‹¤', 'ë¶ˆëŸ‰', 'ì†í•´', 'ë¶ˆì¹œì ˆí•¨',
    'ë¹„ì¶”', 'ë¹„ì¶”í•©ë‹ˆë‹¤', 'ë¬´ì±…ì„', 'ì·¨ì•½', 'ë°˜í’ˆ', 'í™˜ë¶ˆ', 'ë¶ˆë§Œì¡±í–ˆë‹¤', 'ë¶€ì‘ìš©', 'ë¬¸ì œìˆ', 'ë¶ˆí¸í–ˆë‹¤',
    'ì§œì¦ë‚˜ìš”', 'ì‚¬ìš©ë¶ˆê°€', 'ë¨¹ë¨¹', 'ë¶ˆí¸í•œ', 'ëª»ì“°ê² ë‹¤', 'ê³ ì¥', 'í„°ë¬´ë‹ˆì—†', 'ë¶ˆë§Œì¡±ê°', 'ë¶ˆí¸í•¨ì´', 'ë¶ˆì¾Œê°',
    'ë¶ˆí•©ë¦¬', 'ì•…í™”', 'ì•…ì„±', 'ë¶€ì •', 'ë¶€ì ì ˆ', 'ì‹¤ë§ìŠ¤ëŸ¬ì› ë‹¤', 'ì‹¤ë§í–ˆë‹¤', 'í›„íšŒí•œë‹¤', 'ì‹¤ë§í•´ìš”', 'ìµœì•…ì´ë‹¤',
    'ìµœì•…ì´ì—ìš”', 'í˜•í¸ì—†ë‹¤', 'ì—‰ë§ì§„ì°½', 'ì—‰ì„±', 'ê°’ë¹„ì‹¼', 'ë¶ˆì¹œì ˆí–ˆì–´ìš”', 'ë¶ˆí¸í–ˆìŠµë‹ˆë‹¤', 'ë¶€ì •í™•í•¨', 'ë¶€ì£¼ì˜', 'ë¬¸ì œê°€ìˆë‹¤'
]

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ê²€ì¦, ìˆ˜ì§‘, ë¶„ì„) ---
def is_valid_query(query):
    """ 
    ê²€ìƒ‰ì–´ ìœ íš¨ì„± ê²€ì‚¬:
    1. ê³µë°± ì œì™¸ 2ê¸€ì ë¯¸ë§Œ ì°¨ë‹¨
    2. ì˜ë¯¸ ì—†ëŠ” ììŒ/ëª¨ìŒë§Œ ìˆëŠ” ê²½ìš° ì°¨ë‹¨ (ì˜ˆ: ã…‡ã…‡, ã…‹ã…‹)
    3. ë™ì¼ ë¬¸ì ë¬´í•œ ë°˜ë³µ ì°¨ë‹¨
    """
    clean_query = query.replace(' ', '')
    
    # 1. ê¸¸ì´ ì²´í¬
    if len(clean_query) < 2:
        print(f"-> âŒ ê²€ìƒ‰ì–´ '{query}' ì°¨ë‹¨: ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
        return False

    # 2. [ì¶”ê°€ëœ ë¡œì§] ììŒ/ëª¨ìŒë§Œìœ¼ë¡œ êµ¬ì„±ëœ ê²½ìš° ì°¨ë‹¨ (ã„±-ã…, ã…-ã…£)
    # ì •ê·œì‹: í•œê¸€ ììŒ/ëª¨ìŒ ë²”ìœ„ì—ë§Œ í•´ë‹¹í•˜ëŠ”ì§€ ì²´í¬
    if re.fullmatch(r'[ã„±-ã…ã…-ã…£]+', clean_query):
        print(f"-> âŒ ê²€ìƒ‰ì–´ '{query}' ì°¨ë‹¨: ììŒ/ëª¨ìŒë§Œìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return False
        
    # 3. ë™ì¼ ë¬¸ì 3íšŒ ì´ìƒ ë°˜ë³µ ì²´í¬ (aaa, 111 ë“±)
    # (ë‹¨, ë¸Œëœë“œëª…ì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ í•œê¸€ì€ ì œì™¸í•˜ê±°ë‚˜ ê¸°ì¤€ì„ ì™„í™”í•  ìˆ˜ ìˆìŒ)
    repeated_pattern = re.compile(r'([^ê°€-í£])\1{2,}') 
    if repeated_pattern.search(clean_query):
        print(f"-> âŒ ê²€ìƒ‰ì–´ '{query}' ì°¨ë‹¨: ë¬´ì˜ë¯¸í•œ ë¬¸ì ë°˜ë³µ.")
        return False
        
    return True

def load_brand_whitelist():
    """ static/brands.txt íŒŒì¼ì—ì„œ ë¸Œëœë“œ ëª©ë¡ì„ ì½ì–´ì˜µë‹ˆë‹¤. """
    whitelist = set() # ê²€ìƒ‰ ì†ë„ê°€ ë¹ ë¥¸ set ìë£Œêµ¬ì¡° ì‚¬ìš©
    
    # 1. íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
    current_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(current_dir, 'static', 'brands.txt')
    
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    brand = line.strip()
                    if brand:
                        # ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê³  ê³µë°± ì œê±°í•´ì„œ ì €ì¥ (ë¹„êµ ìš©ì´ì„±)
                        whitelist.add(brand.replace(" ", "").lower())
            print(f"-> [í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸] {len(whitelist)}ê°œì˜ ë¸Œëœë“œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"-> [ê²½ê³ ] ë¸Œëœë“œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        print("-> [ê²½ê³ ] brands.txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
        # ë¹„ìƒìš© ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸
        basic_brands = ['ì‚¼ì„±', 'ì• í”Œ', 'ë‚˜ì´í‚¤', 'ë‹¤ì´ì†Œ', 'ì¿ íŒ¡', 'ì¹´ì¹´ì˜¤', 'ë„¤ì´ë²„']
        for b in basic_brands:
            whitelist.add(b)
            
    return whitelist

BRAND_WHITELIST = load_brand_whitelist()

def is_brand_name(query, client_id, client_secret, confidence_threshold=0.15):
    """ 
    ë„¤ì´ë²„ ì‡¼í•‘ APIë¥¼ í†µí•´ ê²€ìƒ‰ì–´ê°€ ë¸Œëœë“œëª…ì¸ì§€ ê²€ì¦ 
    """
    query_norm = query.replace(" ", "").lower()

    # 1. í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì²´í¬
    if query_norm in BRAND_WHITELIST:
        print(f"-> [Pass] '{query}'ëŠ” í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë¸Œëœë“œì…ë‹ˆë‹¤.")
        return True

    # 2. API ê²€ì¦
    print(f"\n--- 0ë‹¨ê³„: '{query}' ë¸Œëœë“œ ê²€ì¦ (API) ---")
    url = "https://openapi.naver.com/v1/search/shop.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    # ì •í™•ë„ìˆœ(sim)ìœ¼ë¡œ ê²€ìƒ‰í•´ì„œ ì—°ê´€ì„±ì„ ë†’ì„
    params = {"query": query, "display": 40, "sort": "sim"}

    try:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        items = response.json().get("items", [])

        if not items:
            print("-> âŒ ì‡¼í•‘ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        brand_list = []
        match_count = 0

        for item in items:
            brand = item.get("brand", "").strip()
            # ë¸Œëœë“œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ë¬´ì‹œ
            if not brand: continue
            
            brand_list.append(brand)
            brand_norm = brand.replace(" ", "").lower()

            # í¬í•¨ ê´€ê³„ í™•ì¸
            if query_norm in brand_norm or brand_norm in query_norm:
                match_count += 1

        total_brands_found = len(brand_list)
        
        #  ê²€ìƒ‰ ê²°ê³¼ëŠ” ìˆëŠ”ë° 'ë¸Œëœë“œ' í•„ë“œê°€ ìˆëŠ” ìƒí’ˆì´ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš° (ì˜ˆ: ì¤‘ê³ ì¥í„° ê¸€ ë“±)
        if total_brands_found == 0:
            print(f"-> âŒ ê²€ìƒ‰ ê²°ê³¼ëŠ” ìˆìœ¼ë‚˜ ë“±ë¡ëœ ë¸Œëœë“œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. (ì¼ë°˜ ëª…ì‚¬ì¼ ê°€ëŠ¥ì„± ë†’ìŒ)")
            return False

        ratio = match_count / total_brands_found
        print(f"-> ê²€ì¦ ê²°ê³¼: ìœ íš¨ ìƒí’ˆ {total_brands_found}ê°œ ì¤‘ {match_count}ê°œ ì¼ì¹˜ ({ratio:.1%})")

        if ratio >= confidence_threshold:
            return True
        else:
            # ìµœë¹ˆê°’ êµ¬ì œ ë¡œì§
            if brand_list:
                most_common = Counter(brand_list).most_common(1)[0][0]
                if query_norm in most_common.replace(" ", "").lower():
                    print(f"-> âš ï¸ ë¹„ìœ¨ ë¯¸ë‹¬ì´ë‚˜ ìµœë¹ˆ ë¸Œëœë“œ('{most_common}')ì™€ ì¼ì¹˜í•˜ì—¬ í†µê³¼.")
                    return True
            
            print(f"-> âŒ ë¸Œëœë“œ ê²€ì¦ ì‹¤íŒ¨ (ê¸°ì¤€ ë¯¸ë‹¬)")
            return False

    except Exception as e:
        print(f"ë¸Œëœë“œ ê²€ì¦ API ì˜¤ë¥˜: {e}")
        # API ì˜¤ë¥˜ ì‹œì—ëŠ” ì–µìš¸í•˜ê²Œ ë§‰íˆëŠ” ê±¸ ë°©ì§€í•˜ê¸° ìœ„í•´ ì¼ë‹¨ í†µê³¼ì‹œí‚¤ê±°ë‚˜, 
        # ì—„ê²©í•˜ê²Œ í•˜ë ¤ë©´ Falseë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤. (ì—¬ê¸°ì„  ì•ˆì „í•˜ê²Œ True ìœ ì§€)
        return True

def fetch_naver_search_results(query, api_type, client_id, client_secret, total_results):
    if api_type == 'blog': url = "https://openapi.naver.com/v1/search/blog.json"
    elif api_type == 'news': url = "https://openapi.naver.com/v1/search/news.json"
    else: return pd.DataFrame()

    df = pd.DataFrame(columns=['title', 'link', 'description', 'channel_name', 'postdate', 'channel_type'])
    headers = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}
    remove_tag = re.compile('<.*?>')
    
    for start in range(1, min(total_results, 1001) + 1, 100):
        try:
            res = requests.get(url, headers=headers, params={"query": query, "display": 100, "start": start})
            res.raise_for_status()
            items = res.json().get("items", [])
            if not items: break
            
            processed = []
            for item in items:
                clean = {
                    'title': re.sub(remove_tag, '', item.get('title', '')),
                    'description': re.sub(remove_tag, '', item.get('description', '')),
                    'link': item.get('link', ''),
                    'channel_type': api_type
                }
                if api_type == 'blog':
                    clean['channel_name'] = re.sub(remove_tag, '', item.get('bloggername', ''))
                    clean['postdate'] = pd.to_datetime(item.get('postdate', ''), format='%Y%m%d', errors='coerce')
                else:
                    clean['channel_name'] = item.get('publisher', '')
                    clean['postdate'] = pd.to_datetime(item.get('pubDate', ''), format='%a, %d %b %Y %H:%M:%S +0900', errors='coerce')
                processed.append(clean)
            
            df = pd.concat([df, pd.DataFrame(processed)], ignore_index=True)
        except: break
        
    df['postdate'] = df['postdate'].dt.normalize()
    return df.dropna(subset=['postdate']).drop_duplicates()

def get_search_trend(query, client_id, client_secret):
    url = "https://openapi.naver.com/v1/datalab/search"
    headers = {
        "X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret,
        "Content-Type": "application/json"
    }
    body = json.dumps({
        "startDate": (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d'),
        "endDate": datetime.now().strftime('%Y-%m-%d'),
        "timeUnit": "month",
        "keywordGroups": [{"groupName": query, "keywords": [query]}]
    })
    try:
        res = requests.post(url, headers=headers, data=body)
        res.raise_for_status()
        data = res.json()
        if not data.get('results'): return pd.DataFrame(columns=['date', 'ratio'])
        df = pd.DataFrame(data['results'][0]['data']).rename(columns={'period': 'date'})
        df['date'] = pd.to_datetime(df['date'])
        return df
    except: return pd.DataFrame(columns=['date', 'ratio'])

# --- ì‹œê°í™” í•¨ìˆ˜ë“¤ ---
def visualize_post_frequency(df, frequency_type='monthly'):
    """ 
    ìµœëŒ€ ê¸°ê°„(12ê°œì›”/6ê°œì›”)ìœ¼ë¡œ ê·¸ë˜í”„ Xì¶•ì„ ê³ ì •í•˜ê³ , 
    ë°ì´í„°ê°€ ì—†ëŠ” ê¸°ê°„ì€ 0ìœ¼ë¡œ ì±„ì›Œì„œ ë³´ì—¬ì¤ë‹ˆë‹¤.
    """
    time_unit = "ì›”ë³„" if frequency_type == 'monthly' else "ì£¼ë³„"
    color = 'darkorange' if frequency_type == 'monthly' else 'purple'
    
    print(f"\n--- 3ë‹¨ê³„ ë¶„ì„: {time_unit} ì–¸ê¸‰ëŸ‰ ì‹œê°í™” (ê¸°ê°„ ê³ ì •) ---")
    
    temp_df = df.copy()
    try:
        temp_df['postdate'] = pd.to_datetime(temp_df['postdate'], errors='coerce').dt.normalize()
        temp_df.dropna(subset=['postdate'], inplace=True)
    except Exception:
        return None 
    
    if temp_df.empty: return None

    now = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    
    # 1. [ê¸°ê°„ ì„¤ì •] ë°ì´í„° ìœ ë¬´ì™€ ìƒê´€ì—†ì´ 12ê°œì›”/6ê°œì›” ì „ë¶€í„° ì‹œì‘
    if frequency_type == 'monthly':
        start_date = now - pd.DateOffset(months=12)
        freq_code = 'MS'
    else:
        start_date = now - pd.DateOffset(months=6)
        freq_code = 'W'

    # 2. [í•„í„°ë§] í•´ë‹¹ ê¸°ê°„ ë‚´ ë°ì´í„°ë§Œ ì¶”ì¶œ
    df_filtered = temp_df[(temp_df['postdate'] >= start_date) & (temp_df['postdate'] <= now)].copy()
    
    # 3. [ì „ì²´ ë²”ìœ„ ìƒì„±] ë¹ˆ ê¸°ê°„ì„ ì±„ìš°ê¸° ìœ„í•œ ë‚ ì§œ ì¸ë±ìŠ¤
    full_date_range = pd.date_range(start=start_date, end=now, freq=freq_code)
    
    if frequency_type == 'monthly':
        # ì›”ë³„ ì§‘ê³„
        df_filtered['period_dt'] = df_filtered['postdate'].dt.to_period('M').dt.to_timestamp()
        counts_raw = df_filtered['period_dt'].value_counts().sort_index()
        
        # ë¹ˆ ê³³ 0ìœ¼ë¡œ ì±„ìš°ê¸°
        full_counts = counts_raw.reindex(full_date_range, fill_value=0)
        
        # ë¼ë²¨: YYYY-MM
        full_counts.index = full_counts.index.strftime('%Y-%m')
        rotation_angle = 45

    elif frequency_type == 'weekly':
        # ì£¼ë³„ ì§‘ê³„
        df_resample = df_filtered.set_index('postdate')
        counts_resampled = df_resample.resample('W').size()
        
        # ë¹ˆ ê³³ 0ìœ¼ë¡œ ì±„ìš°ê¸°
        full_counts = counts_resampled.reindex(full_date_range, fill_value=0)
        
        # ë¼ë²¨: 0000ë…„ 0ì›” 0ì£¼ì°¨
        new_labels = []
        for date in full_counts.index:
            first_day = date.replace(day=1)
            dom = date.day
            adjusted_dom = dom + first_day.weekday()
            w = int(np.ceil(adjusted_dom/7.0))
            if w > 5: w = 5
            new_labels.append(f"{date.year}ë…„ {date.month}ì›” {w}ì£¼ì°¨")
            
        full_counts.index = new_labels
        rotation_angle = 45
        
    else:
        return None

    if full_counts.empty: return None

    # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    fig = plt.figure(figsize=(15 if frequency_type == 'weekly' else 12, 6))
    sns.lineplot(x=full_counts.index, y=full_counts.values, marker='o', color=color)
    
    # ì œëª© ì„¤ì •
    period_msg = "ìµœê·¼ 12ê°œì›”" if frequency_type == 'monthly' else "ìµœê·¼ 6ê°œì›”"
    plt.title(f'{period_msg} ì–¸ê¸‰ëŸ‰ ì¶”ì´ ({time_unit})', fontsize=16)
    
    if frequency_type == 'weekly':
        plt.xlabel(f'ì–¸ê¸‰ {time_unit} (ì£¼ì°¨)', fontsize=12) 
    else:
        plt.xlabel(f'ì–¸ê¸‰ {time_unit}', fontsize=12) 
        
    plt.ylabel('ì´ ì–¸ê¸‰ëŸ‰', fontsize=12)

    plt.xticks(full_counts.index, rotation=rotation_angle) 
    plt.grid(axis='y', linestyle='--')
    plt.tight_layout()
    
    print(f"-> í†µí•© ë°ì´í„° {time_unit} ì–¸ê¸‰ëŸ‰ ì‹œê°í™” í”Œë¡¯ ê°ì²´ ìƒì„± ì™„ë£Œ.")
    return fig

def visualize_combined_trend(total_df, trend_df):
    if trend_df.empty: return None
    temp = total_df.copy()
    temp['month'] = temp['postdate'].dt.to_period('M').dt.to_timestamp()
    mention_counts = temp['month'].value_counts().reset_index()
    mention_counts.columns = ['date', 'mention_count']
    
    trend_df['date'] = trend_df['date'].dt.to_period('M').dt.to_timestamp()
    combined = pd.merge(trend_df, mention_counts, on='date', how='left').fillna(0)
    if combined.empty: return None
    combined['mention_ratio'] = (combined['mention_count'] / combined['mention_count'].max() * 100) if combined['mention_count'].max() > 0 else 0

    fig, ax1 = plt.subplots(figsize=(12, 6))
    sns.lineplot(x=combined.index, y=combined['ratio'], ax=ax1, color='tab:red', marker='o', label='ê²€ìƒ‰ëŸ‰')
    ax2 = ax1.twinx()
    sns.lineplot(x=combined.index, y=combined['mention_ratio'], ax=ax2, color='tab:blue', marker='s', label='ì–¸ê¸‰ëŸ‰')
    plt.title('ê²€ìƒ‰ëŸ‰ vs ì–¸ê¸‰ëŸ‰ êµì°¨ ë¶„ì„', fontsize=16)
    ax1.set_xticks(range(len(combined)))
    ax1.set_xticklabels(combined['date'].dt.strftime('%Y-%m'), rotation=45)
    fig.tight_layout()
    return fig

def visualize_sentiment_word_clouds(df, pos_words, neg_words):
    if df.empty: return None, None, None, pd.DataFrame()
    text = ' '.join(df['description'].astype(str).tolist())
    cleaned = re.sub('[^ê°€-í£\s]', '', text).lower()
    
    def make_wc(words, title, cmap):
        counts = {w: len(re.findall(rf'\b{w}\b', cleaned)) for w in words}
        counts = {k: v for k, v in counts.items() if v > 0}
        if not counts: return None
        wc = WordCloud(font_path=FONT_PATH, width=800, height=400, background_color='white', colormap=cmap)
        wc.generate_from_frequencies(counts)
        fig = plt.figure(figsize=(12, 7))
        plt.imshow(wc, interpolation='bilinear')
        plt.axis('off')
        plt.title(title, fontsize=16)
        plt.tight_layout()
        return fig

    pos_fig = make_wc(pos_words, 'ê¸ì • í‚¤ì›Œë“œ', 'YlGn')
    neg_fig = make_wc(neg_words, 'ë¶€ì • í‚¤ì›Œë“œ', 'Reds_r')
    
    # í†µí•© í‚¤ì›Œë“œ ë° ìƒìœ„ ë¦¬ìŠ¤íŠ¸
    all_counts = {w: len(re.findall(rf'\b{w}\b', cleaned)) for w in pos_words + neg_words}
    all_counts = dict(sorted(all_counts.items(), key=lambda x: x[1], reverse=True)[:20])
    all_fig = make_wc(list(all_counts.keys()), 'í†µí•© í‚¤ì›Œë“œ', 'plasma')
    
    top_k = pd.DataFrame(all_counts.items(), columns=['í‚¤ì›Œë“œ', 'ì–¸ê¸‰ íšŸìˆ˜']) if all_counts else pd.DataFrame()
    return pos_fig, neg_fig, all_fig, top_k

def visualize_competitor_mention_comparison(own_query, own_df, competitor_query, competitor_df):
    """ 
    ìì‚¬ vs ê²½ìŸì‚¬ ì›”ë³„ ì–¸ê¸‰ëŸ‰ì„ ë¹„êµí•©ë‹ˆë‹¤.
    (ìµœê·¼ 12ê°œì›” ê³ ì •, ë¹ˆ ë‹¬ì€ 0ìœ¼ë¡œ ì±„ì›€, ë‚ ì§œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ ë³´ì¥)
    """
    print(f"\n--- 9ë‹¨ê³„ ë¶„ì„: ìì‚¬({own_query}) vs ê²½ìŸì‚¬({competitor_query}) ì›”ë³„ ì–¸ê¸‰ëŸ‰ ë¹„êµ ---")

    # ë‘˜ ë‹¤ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê·¸ë¦´ ìˆ˜ ì—†ìŒ
    if own_df.empty and competitor_df.empty:
        print("ê²½ê³ : ë¹„êµí•  ë°ì´í„°ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    # 1. ê¸°ì¤€ ê¸°ê°„ ì„¤ì • (ìµœê·¼ 12ê°œì›”)
    now = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    start_date = now - pd.DateOffset(months=12)
    
    # 2. ì™„ì „í•œ ë‚ ì§œ ì¸ë±ìŠ¤ ìƒì„± (ì´ê²Œ ìˆì–´ì•¼ ìˆœì„œê°€ ì•ˆ ì„ì´ê³  0ìœ¼ë¡œ ì±„ì›Œì§)
    full_date_range = pd.date_range(start=start_date, end=now, freq='MS')

    # ë‚´ë¶€ ì§‘ê³„ í•¨ìˆ˜
    def get_monthly_counts(df):
        temp_df = df.copy()
        if temp_df.empty: 
            return pd.Series(0, index=full_date_range) # ë°ì´í„° ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ê½‰ ì±„ìš´ ì‹œë¦¬ì¦ˆ ë°˜í™˜
        
        try:
            temp_df['postdate'] = pd.to_datetime(temp_df['postdate'], errors='coerce').dt.normalize()
            temp_df.dropna(subset=['postdate'], inplace=True)
            
            # ê¸°ê°„ ë‚´ ë°ì´í„° í•„í„°ë§
            mask = (temp_df['postdate'] >= start_date) & (temp_df['postdate'] <= now)
            filtered = temp_df[mask]
            
            # ì›”ë³„ ë¦¬ìƒ˜í”Œë§ (MS: Month Start)
            # set_index -> resample -> size ê³¼ì •ì„ ê±°ì¹˜ë©´ ë‚ ì§œìˆœìœ¼ë¡œ ìë™ ì •ë ¬ë¨
            counts = filtered.set_index('postdate').resample('MS').size()
            
            # ë¹ˆ ë‹¬ì„ 0ìœ¼ë¡œ ì±„ìš°ê¸° (Reindex)
            return counts.reindex(full_date_range, fill_value=0)
            
        except Exception as e:
            print(f"ë°ì´í„° ì§‘ê³„ ì¤‘ ì˜¤ë¥˜: {e}")
            return pd.Series(0, index=full_date_range)

    # 3. ìì‚¬/ê²½ìŸì‚¬ ë°ì´í„° ì§‘ê³„
    own_counts = get_monthly_counts(own_df)
    comp_counts = get_monthly_counts(competitor_df)
    
    # 4. ë°ì´í„°í”„ë ˆì„ í†µí•©
    combined = pd.DataFrame({
        own_query: own_counts,
        competitor_query: comp_counts
    })
    
    # 5. ê·¸ë˜í”„ ê·¸ë¦¬ê¸° (Figure ê°ì²´ ìƒì„±)
    fig = plt.figure(figsize=(10, 5))
    
    # Xì¶• ë¼ë²¨ì„ ë³´ê¸° ì¢‹ê²Œ ë³€í™˜ (YYYY-MM)
    x_labels = combined.index.strftime('%Y-%m')

    # ë¼ì¸ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    sns.lineplot(data=combined, markers=True, dashes=False)

    plt.title(f'{own_query} vs {competitor_query} ì›”ë³„ ì–¸ê¸‰ëŸ‰ ë¹„êµ (ìµœê·¼ 12ê°œì›”)', fontsize=14) 
    plt.xlabel('ì›”', fontsize=11) 
    plt.ylabel('ì´ ì–¸ê¸‰ëŸ‰', fontsize=11)
    
    # Xì¶• ì„¤ì • (ì •ë ¬ëœ ë‚ ì§œ ì¸ë±ìŠ¤ ì‚¬ìš©)
    plt.xticks(ticks=combined.index, labels=x_labels, rotation=45) 
    
    plt.legend(fontsize=11)
    plt.grid(axis='y', linestyle='--')
    plt.tight_layout()
    
    print("-> ìì‚¬/ê²½ìŸì‚¬ ì–¸ê¸‰ëŸ‰ ë¹„êµ ì‹œê°í™” í”Œë¡¯ ê°ì²´ ìƒì„± ì™„ë£Œ.")
    return fig

# --- AI ë¦¬í¬íŠ¸ ìƒì„± ---
def generate_smart_report(query, total, sent_label, pos_score, keywords, outbreak, trend_ok, freq_date, change_rate, api_key):
    key_str = ", ".join([k['í‚¤ì›Œë“œ'] for k in keywords[:5]]) if keywords else "ì—†ìŒ"
    outbreak_text = f"{outbreak[0]}" if outbreak else "ì—†ìŒ"
    
    prompt = f"""
        ë‹¹ì‹ ì€ 'ê²€ìƒ‰ëŸ‰(ê´€ì‹¬ë„)'ê³¼ 'ì–¸ê¸‰ëŸ‰(ë²„ì¦ˆëŸ‰)'ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” ë¸Œëœë“œ í‰íŒ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì œê³µëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{query}' ë¸Œëœë“œì˜ í˜„í™©ì„ ì§„ë‹¨í•˜ê³ , ë…¼ë¦¬ì ì¸ ë§ˆì¼€íŒ… ì†”ë£¨ì…˜ì„ ì œì‹œí•˜ì„¸ìš”.

        ğŸ“Š [ë°ì´í„° ê°œìš”]
        - ì´ ì–¸ê¸‰ëŸ‰(Buzz): {total}ê±´ (ì‹œì¥ì˜ ë°˜ì‘ í¬ê¸°)
        - ì–¸ê¸‰ëŸ‰ ì¦ê°ë¥ : {change_rate} (ìµœê·¼ ì¶”ì„¸)
        - ìµœë‹¤ ì–¸ê¸‰ì¼: {freq_date}
        - ì´ìŠˆ í™•ì‚° í¬ì¸íŠ¸: {outbreak_text} (ê²€ìƒ‰ëŸ‰ ê¸‰ì¦ ì‹œì )
        - ì—¬ë¡  ê°ì„±: {sent_label} (ê¸ì • {pos_score}%)
        - í•µì‹¬ í‚¤ì›Œë“œ: {key_str}

        ğŸ“ [ì‘ì„± ê°€ì´ë“œ (500ì ë‚´ì™¸, ê°œì¡°ì‹)]
        ì•„ë˜ 4ê°€ì§€ ëª©ì°¨ì— ë§ì¶° ë¶„ì„í•˜ë˜, ë‹¨ìˆœ ìˆ˜ì¹˜ ë‚˜ì—´ì´ ì•„ë‹Œ 'ì¸ì‚¬ì´íŠ¸' ìœ„ì£¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

        1. ğŸ” [ê´€ì‹¬ë„-í™•ì‚°ì„± êµì°¨ ë¶„ì„]
        - 'ì´ìŠˆ í™•ì‚°(ê²€ìƒ‰ëŸ‰ ê¸‰ì¦)'ê³¼ 'ì´ ì–¸ê¸‰ëŸ‰'ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ í˜„ì¬ ë‹¨ê³„ë¥¼ ì •ì˜í•˜ì„¸ìš”.
        - (ì˜ˆ: ê²€ìƒ‰ëŸ‰ì´ ì„ í–‰í•˜ê³  ì–¸ê¸‰ëŸ‰ì´ ë”°ë¼ì˜¤ëŠ” 'ìƒìŠ¹ê¸°'ì¸ì§€, ê²€ìƒ‰ ì—†ì´ ì–¸ê¸‰ë§Œ ë§ì€ 'ë°”ì´ëŸ´ ë‹¨ê³„'ì¸ì§€ ì§„ë‹¨)

        2. ğŸ—£ï¸ [ì—¬ë¡ ì˜ ì§ˆì  ì§„ë‹¨]
        - ê¸ì • ì—¬ë¡ ({pos_score}%)ì˜ êµ¬ì²´ì ì¸ ì„±ê²©ì„ í‚¤ì›Œë“œì™€ ì—°ê´€ ì§€ì–´ í•´ì„í•˜ì„¸ìš”.
        - ë‹¨ìˆœí•œ í˜¸ê°ì¸ì§€, êµ¬ë§¤ë¡œ ì´ì–´ì§€ëŠ” ì‹ ë¢°ì¸ì§€, í˜¹ì€ ë¶€ì •ì  ì´ìŠˆ ë°©ì–´ì¸ì§€ ë¶„ì„í•˜ì„¸ìš”.

        3. ğŸ”‘ [íŠ¸ë Œë“œ ë§¥ë½(Context)]
        - ë„ì¶œëœ í‚¤ì›Œë“œ({key_str})ë“¤ì´ ì™œ ì´ ì‹œì ì— ë“±ì¥í–ˆëŠ”ì§€, ì†Œë¹„ìì˜ ì–´ë–¤ ë‹ˆì¦ˆ(Needs)ë¥¼ ë°˜ì˜í•˜ëŠ”ì§€ ì„¤ëª…í•˜ì„¸ìš”.

        4. ğŸ’¡ [Actionable Strategy]
        - ìœ„ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ êµ¬ì²´ì ì¸ í–‰ë™ ì „ëµì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì œì•ˆí•˜ì„¸ìš”.
        - (High Search/Low Mentionì¼ ê²½ìš° -> "ì •ë³´ì„± ì½˜í…ì¸  ë³´ê°•ìœ¼ë¡œ êµ¬ë§¤ ì „í™˜ ìœ ë„")
        - (Low Search/High Mentionì¼ ê²½ìš° -> "ì´ë²¤íŠ¸ì„± ê±°í’ˆ ì£¼ì˜ ë° ë¸Œëœë“œ ì§„ì •ì„± ê°•í™”")
        """
    if api_key:
        try:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?key={api_key}"
            res = requests.post(url, headers={'Content-Type': 'application/json'}, data=json.dumps({"contents": [{"parts": [{"text": prompt}]}]}))
            if res.status_code == 200: return markdown.markdown(res.json()['candidates'][0]['content']['parts'][0]['text'])
        except: pass
    return markdown.markdown(f"### ë¶„ì„ ìš”ì•½\n- ì´ ì–¸ê¸‰ëŸ‰: {total}ê±´\n- ê°ì„±: {sent_label}\n- ì£¼ìš” í‚¤ì›Œë“œ: {key_str}")

# --- í—¬í¼ í•¨ìˆ˜ë“¤ ---
def get_month_week(date_obj):
    """ 
    ë‚ ì§œ(date_obj)ë¥¼ ë°›ì•„ì„œ í•´ë‹¹ ì›”ì˜ ëª‡ ë²ˆì§¸ ì£¼ì¸ì§€ ê³„ì‚°í•©ë‹ˆë‹¤.
    (ìµœëŒ€ 5ì£¼ì°¨ê¹Œì§€ë§Œ ë‚˜ì˜¤ë„ë¡ ë³´ì •)
    """
    # ë§¤ì›” 1ì¼ ì°¾ê¸°
    first_day = date_obj.replace(day=1)
    
    # ë‚ ì§œ(ì¼) + 1ì¼ì˜ ìš”ì¼ ì¸ë±ìŠ¤(ì›”=0, ì¼=6)ë¥¼ ë”í•´ì„œ ì£¼ì°¨ ê³„ì‚°
    dom = date_obj.day
    adjusted_dom = dom + first_day.weekday()
    
    # ì˜¬ë¦¼ ì²˜ë¦¬í•˜ì—¬ ì£¼ì°¨ ê³„ì‚°
    week_num = int(np.ceil(adjusted_dom/7.0))
    
    # 6ì£¼ì°¨ê°€ ë‚˜ì˜¤ë©´ 5ì£¼ì°¨ë¡œ ê°•ì œ í¸ì…
    if week_num > 5: 
        return 5
    return week_num

def find_outbreak_weeks(trend_df, change_threshold=0.3):
    if trend_df.empty: return []
    
    filtered = trend_df.sort_values('date')
    
    # ì—ëŸ¬ ë°©ì§€: ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜
    filtered['ratio'] = pd.to_numeric(filtered['ratio'], errors='coerce')
    
    # ë³€í™”ìœ¨(pct_change) ê³„ì‚°
    filtered['change'] = filtered['ratio'].pct_change()
    
    # ê¸‰ì¦í•œ êµ¬ê°„(threshold ì´ˆê³¼) ì°¾ê¸°
    outbreak = filtered[filtered['change'] > change_threshold]
    
    results = []
    for _, row in outbreak.iterrows():
        m = row['date'].month
        w = get_month_week(row['date']) 
        
        results.append(f"{row['date'].year}ë…„ {m}ì›” {w}ì£¼ì°¨")
        
    return results

def calculate_lexicon_score(text):
    cleaned = re.sub('[^ê°€-í£\s]', '', text).lower()
    pos = sum(len(re.findall(rf'\b{w}\b', cleaned)) for w in POSITIVE_WORDS)
    neg = sum(len(re.findall(rf'\b{w}\b', cleaned)) for w in NEGATIVE_WORDS)
    return (pos / (pos + neg) * 100) if (pos + neg) > 0 else 50.0

def classify_sentiment(score):
    return f"ğŸŸ¢ ê¸ì •" if score > 60 else f"ğŸ”´ ë¶€ì •"

def calculate_key_metrics(df):
    if df.empty: return 'N/A', 'N/A'
    most_freq = df['postdate'].mode()[0].strftime('%Y-%m-%d')
    d1 = df[(df['postdate'] >= datetime.now() - timedelta(30))].shape[0]
    d2 = df[(df['postdate'] >= datetime.now() - timedelta(60)) & (df['postdate'] < datetime.now() - timedelta(30))].shape[0]
    rate = f"{((d1-d2)/d2)*100:+.1f}%" if d2 > 0 else "0.0%"
    return most_freq, rate

# ==================================================================
# [3] ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (Flask í˜¸ì¶œìš©)
# ==================================================================
def run_full_analysis(search_query, competitor_query, client_id, client_secret, max_results, static_folder):
    results = {
        "query": search_query, "competitor_query": competitor_query,
        "status": "FAILURE", "message": "ì˜¤ë¥˜", "visualization_urls": {}
    }

    if not is_valid_query(search_query):
        results["message"] = "ê²€ìƒ‰ì–´ ì˜¤ë¥˜"
        return results

    # 1. ë°ì´í„° ìˆ˜ì§‘
    blog = fetch_naver_search_results(search_query, 'blog', client_id, client_secret, max_results)
    news = fetch_naver_search_results(search_query, 'news', client_id, client_secret, max_results)
    total = pd.concat([blog, news], ignore_index=True)
    trend = get_search_trend(search_query, client_id, client_secret)

    if total.empty:
        results["status"] = "INSUFFICIENT_DATA"
        return results

    comp_df = pd.DataFrame()
    if competitor_query:
        cb = fetch_naver_search_results(competitor_query, 'blog', client_id, client_secret, max_results)
        cn = fetch_naver_search_results(competitor_query, 'news', client_id, client_secret, max_results)
        comp_df = pd.concat([cb, cn], ignore_index=True)

    # 2. ì§€í‘œ ê³„ì‚°
    desc = ' '.join(total['description'].astype(str).tolist())
    score = calculate_lexicon_score(desc)
    sent_label = classify_sentiment(score)
    freq_date, change_rate = calculate_key_metrics(total)
    outbreak = find_outbreak_weeks(trend)

    # 3. ì‹œê°í™” ìƒì„± (âš ï¸ ê³ ìœ  ID ì‚¬ìš©)
    unique_id = str(uuid.uuid4())[:8]
    urls = {}
    
    pos_p, neg_p, all_p, top_k = visualize_sentiment_word_clouds(total, POSITIVE_WORDS, NEGATIVE_WORDS)
    urls["positive_wordcloud"] = save_and_get_url(lambda: pos_p, "sentiment_pos_wc.png", static_folder, unique_id)
    urls["negative_wordcloud"] = save_and_get_url(lambda: neg_p, "sentiment_neg_wc.png", static_folder, unique_id)
    urls["combined_wordcloud"] = save_and_get_url(lambda: all_p, "sentiment_wc.png", static_folder, unique_id)
    
    urls["monthly_frequency"] = save_and_get_url(lambda: visualize_post_frequency(total, 'monthly'), "freq_month.png", static_folder, unique_id)
    urls["weekly_frequency"] = save_and_get_url(lambda: visualize_post_frequency(total, 'weekly'), "freq_week.png", static_folder, unique_id)
    urls["combined_trend"] = save_and_get_url(lambda: visualize_combined_trend(total, trend), "trend_cross.png", static_folder, unique_id)

    if not comp_df.empty:
        urls["competitor_comparison"] = save_and_get_url(
            lambda: visualize_competitor_mention_comparison(search_query, total, competitor_query, comp_df),
            "comp_compare.png", static_folder, unique_id
        )

    # 4. ê²°ê³¼ ë°˜í™˜
    results.update({
        "status": "SUCCESS", "message": "ì™„ë£Œ",
        "total_mentions": len(total),
        "most_frequent_date": freq_date,
        "mention_change_rate": change_rate,
        "positive_percentage": int(score),
        "final_sentiment_label": sent_label,
        "top_keywords": top_k.to_dict('records'),
        "visualization_urls": urls,
        "post_list": total[['title', 'postdate', 'channel_name', 'link']].rename(columns={'postdate': 'date', 'channel_name': 'author'}).to_dict('records'),
        "outbreak_weeks": outbreak
    })

    results["ai_report"] = generate_smart_report(
        search_query, len(total), sent_label, int(score), results["top_keywords"], 
        outbreak, not trend.empty, freq_date, change_rate, 
        os.getenv("GEMINI_API_KEY")
    )
    
    return results

if __name__ == "__main__":
    print("Flask ë°±ì—”ë“œ ëª¨ë“ˆì…ë‹ˆë‹¤.")