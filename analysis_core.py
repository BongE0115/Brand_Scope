import os
import pandas as pd
import json
import re
import requests 
from datetime import datetime, timedelta
from collections import Counter 
import gc 
import matplotlib.pyplot as plt 
import matplotlib
import requests.utils

# âš ï¸ [ì¶”ê°€] Flask ì—°ë™ ë° ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•´ í•„ìˆ˜ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import io 
import numpy as np 

# íƒœê·¸ í´ë¼ìš°ë“œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from wordcloud import WordCloud 


# ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
import seaborn as sns

from matplotlib import font_manager, rc 


def save_and_get_url(plot_func, filename, static_folder):
    """ Matplotlib ê·¸ë˜í”„ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  URLì„ ë°˜í™˜í•©ë‹ˆë‹¤. """
    if not static_folder:
        return None

    try:
        # --- âš ï¸ [ìˆ˜ì •] ---
        # 1. 'static' í´ë” ì•ˆì— 'img' í´ë” ê²½ë¡œë¥¼ ë§Œë“­ë‹ˆë‹¤.
        img_save_path = os.path.join(static_folder, 'img')

        # 2. 'static/img' í´ë”ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        if not os.path.exists(img_save_path):
            os.makedirs(img_save_path)
        # ---------------------

        plot_object = plot_func()

        if plot_object is None:
             return None

        # 3. íŒŒì¼ ì €ì¥ ê²½ë¡œë¥¼ 'static/img/íŒŒì¼ì´ë¦„'ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
        filepath = os.path.join(img_save_path, filename) # ğŸ‘ˆ static/img/íŒŒì¼ì´ë¦„

        if os.path.exists(filepath):
            os.remove(filepath)

        plot_object.savefig(filepath, dpi=100)
        plt.close('all')        # 4. ë¸Œë¼ìš°ì €ê°€ ì´ë¯¸ì§€ë¥¼ ìš”ì²­í•  URLë„ '/static/img/íŒŒì¼ì´ë¦„'ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
        return f"/static/img/{filename}" # ğŸ‘ˆ /static/img/íŒŒì¼ì´ë¦„

    except Exception as e:
        print(f"-> âŒ ì‹œê°í™” ì €ì¥ ë° URL ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({filename}): {e}")
        plt.close('all')
        return None

# ----------------------------------------------------
# --- âš ï¸ì„¤ì • (Configuration) ---
NAVER_CLIENT_ID = "keeE_3_zOuG8ndn5AdQd" 
NAVER_CLIENT_SECRET = "FVUwmNaHst" 
# ----------------------------------------------------

# ğŸš¨ğŸš¨ ë°ì´í„° ìˆ˜ì§‘ ê°¯ìˆ˜ ì„¤ì • ë¶€ë¶„ ğŸš¨ğŸš¨
MAX_RESULTS_PER_API = 1000 


# --- ìˆœìˆ˜ Python ê°ì„± ì‚¬ì „ ì •ì˜ (Lexicon) ---
POSITIVE_WORDS = [
    'ì¢‹ì•„ìš”', 'ìµœê³ ', 'ë§Œì¡±', 'ì¶”ì²œ', 'ê°•ë ¥ì¶”ì²œ', 'ëŒ€ë°•', 'ì˜ˆì˜', 'í¸ì•ˆ', 'í–‰ë³µ', 'ê°ì‚¬', 
    'ê¸°ì¨', 'í›Œë¥­', 'ì‚¬ë‘', 'ì¬ë¯¸', 'ì¦ê±°ì›€', 'ì„±ê³µ', 'í•©ê²©', 'ì„ ë¬¼', 'ë”°ëœ»', 'ë°ì€', 
    'ì™„ë²½', 'ì¸ìƒí…œ', 'ê°€ì„±ë¹„', 'ì°©í•œ', 'ë†€ë', 'ê¸°ëŒ€', 'ë³´ëŒ', 'ê¹¨ë—', 'ì‹±ê·¸ëŸ¬', 'í”„ë¦¬ë¯¸ì—„'
]

NEGATIVE_WORDS = [
    'ë³„ë¡œ', 'ì‹¤ë§', 'ì•„ì‰¬ì›€', 'ë‚˜ì˜', 'ë¶ˆë§Œ', 'ë¶ˆí¸', 'ìµœì•…', 'í˜ë“¤', 'ê±±ì •', 'ë¬¸ì œ', 
    'ì–´ë ¤ì›€', 'ë¶€ì¡±', 'ì‹¤íŒ¨', 'ë‚­ë¹„', 'ì“°ë ˆê¸°', 'ë¹„ì‹¸', 'ë–¨ì–´ì§', 'ì‹¤ë§ìŠ¤ëŸ½', 'ì§€ë£¨', 'í›„íšŒ',
    'ì´ìƒ', 'ì˜¤ë¥˜', 'ì§œì¦', 'ê³ í†µ', 'ë…¼ë€', 'ë¶€ì •ì ', 'ì•½ì ', 'ê²°í•¨', 'ë³µì¡', 'ë¶ˆí•„ìš”'
]

# --- í°íŠ¸ ì„¤ì • (í†µí•© ë¡œì§) ---
# 1. ì‹œìŠ¤í…œì— ì„¤ì¹˜ëœ í•œê¸€ í°íŠ¸ ì°¾ê¸°
def get_korean_font():
    """ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸(Malgun Gothic, AppleGothic, NanumGothic ë“±)ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    font_names = ['Malgun Gothic', 'AppleGothic', 'NanumGothic', 'Noto Sans CJK JP']
    for font_name in font_names:
        font_path = font_manager.findfont(font_name)
        if font_path:
            return font_name, font_path
    return None, None

korean_font_name, korean_font_path = get_korean_font()

if korean_font_name:
    # 2. í°íŠ¸ ìºì‹œ ì´ˆê¸°í™” (í•„ìš”ì‹œ)
    try:
        # ì´ ë¶€ë¶„ì€ í™˜ê²½ì— ë”°ë¼ ìºì‹œê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        pass
    except Exception:
        pass
    
    # 3. Matplotlibì— í°íŠ¸ ì„¤ì • ì ìš©
    # âš ï¸ [ìœ ì§€] font_manager.fontManager.addfont í˜¸ì¶œì€ í™˜ê²½ì— ë”°ë¼ ì—ëŸ¬ë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆì§€ë§Œ, 
    # ì•ˆì •ì ì¸ ì‚¬ìš©ì í™˜ê²½ì„ ê°€ì •í•˜ê³  ìœ ì§€í•©ë‹ˆë‹¤.
    # font_manager.fontManager.addfont(korean_font_path)
    rc('font', family=korean_font_name)
    print(f"-> âœ… í•œê¸€ í°íŠ¸ '{korean_font_name}' ì„¤ì • ì™„ë£Œ.")
    FONT_PATH = korean_font_path # ì›Œë“œí´ë¼ìš°ë“œìš© ê²½ë¡œ ì„¤ì •
else:
    print("-> âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    FONT_PATH = None
    
plt.rcParams['axes.unicode_minus'] = False # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
# ----------------------------------------------------

# ----------------------------------------------------
# --- ê²€ìƒ‰ì–´ ìœ íš¨ì„± ë° ë¸Œëœë“œëª… ê²€ì¦ í•¨ìˆ˜ ---
# ----------------------------------------------------

def is_valid_query(query):
    """ ë¬´ì˜ë¯¸í•œ ë°˜ë³µ ë¬¸ìì—´, ë„ˆë¬´ ì§§ì€ ë¬¸ìì—´ ë“±ì„ ê±¸ëŸ¬ë‚´ ë¶„ì„ì„ ìœ„í•œ ê²€ìƒ‰ì–´ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤. """
    if len(query.replace(' ', '')) < 2:
        print("-> âŒ ê²€ìƒ‰ì–´ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. (ê³µë°± ì œì™¸ 2ì ë¯¸ë§Œ)")
        return False
    # âš ï¸ [ìˆ˜ì •] ì •ê·œì‹ íŒ¨í„´ì´ ì¼ë¶€ í™˜ê²½ì—ì„œ ì˜¤ë¥˜ë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆì–´, Raw Stringìœ¼ë¡œ ëª…ì‹œí•©ë‹ˆë‹¤.
    repeated_pattern = re.compile(r'([ê°€-í£a-zA-Z0-9])\1{3,}')
    if repeated_pattern.search(query):
        print("-> âŒ ë™ì¼í•œ ë¬¸ìê°€ 4íšŒ ì´ìƒ ë°˜ë³µë˜ëŠ” íŒ¨í„´ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. (ë¬´ì˜ë¯¸í•œ ê²€ìƒ‰ì–´ë¡œ íŒë‹¨)")
        return False
    return True

def is_brand_name(query, client_id, client_secret, confidence_threshold=0.6):
    """ ë„¤ì´ë²„ ì‡¼í•‘ APIë¥¼ ì´ìš©í•´ ê²€ìƒ‰ì–´ê°€ ë¸Œëœë“œëª…ì¸ì§€ êµì°¨ ê²€ì¦í•©ë‹ˆë‹¤. """
    print(f"\n--- 0ë‹¨ê³„: '{query}'ê°€ ë¸Œëœë“œëª…ì¸ì§€ ê²€ì¦ ì‹œì‘ ---")
    url = "https://openapi.naver.com/v1/search/shop.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {"query": query, "display": 20} 

    try:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        data = response.json()
        items = data.get("items", [])

        if not items:
            print("-> ì‡¼í•‘ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ë¸Œëœë“œëª…ìœ¼ë¡œ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        brands = [item.get("brand") for item in items if item.get("brand")]
        if not brands:
            print("-> ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë¸Œëœë“œëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        brands = [b.strip() for b in brands] # âš ï¸ [ì¶”ê°€] ë¸Œëœë“œëª… ì•ë’¤ ê³µë°± ì œê±°
        
        brand_counts = Counter(brands)
        most_common_brand, count = brand_counts.most_common(1)[0]
        dominance_ratio = count / len(brands)
        
        is_dominant = dominance_ratio >= confidence_threshold
        # âš ï¸ [ìˆ˜ì •] ê²€ìƒ‰ì–´ì™€ ë¸Œëœë“œëª…ì„ ë¹„êµí•  ë•Œ, ì†Œë¬¸ìí™”í•˜ì—¬ ì •í™•í•œ ë¹„êµë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        is_query_matched = most_common_brand.lower() == query.lower() 

        print(f"-> ê°€ì¥ ë§ì´ ë…¸ì¶œëœ ë¸Œëœë“œ: '{most_common_brand}' (ë¹„ì¤‘: {dominance_ratio:.2%})")

        if is_dominant and is_query_matched:
            print(f"-> âœ… ë¸Œëœë“œëª… ì¼ì¹˜ ë° ë¹„ì¤‘({dominance_ratio:.2%}) í†µê³¼.")
            return True
        else:
            reason = []
            if not is_dominant:
                reason.append(f"ë¸Œëœë“œ ë¹„ì¤‘({dominance_ratio:.2%})ì´ ê¸°ì¤€ì¹˜({confidence_threshold:.0%}) ë¯¸ë§Œ")
            if not is_query_matched:
                reason.append(f"ê°€ì¥ ì§€ë°°ì ì¸ ë¸Œëœë“œ('{most_common_brand}')ê°€ ê²€ìƒ‰ì–´ì™€ ë¶ˆì¼ì¹˜")
            
            print(f"-> âŒ ë¸Œëœë“œëª…ìœ¼ë¡œ íŒë‹¨í•˜ê¸° ì–´ë ¤ì›€ ({', '.join(reason)}).")
            return False

    except requests.exceptions.RequestException as e:
        # âš ï¸ [ìˆ˜ì •] ì˜¤ë¥˜ ë°œìƒ ì‹œ 500 ì—ëŸ¬ë¥¼ ë°˜í™˜í•´ì•¼ í•˜ë¯€ë¡œ, í•¨ìˆ˜ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.
        print(f"ì‡¼í•‘ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

# ----------------------------------------------------
# --- ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜ ---
# ----------------------------------------------------

def fetch_naver_search_results(query, api_type, client_id, client_secret, total_results):
    """ Blog ë˜ëŠ” News ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. """
    print(f"\n--- 1ë‹¨ê³„: '{api_type.capitalize()}' ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {total_results}ê±´) ---")

    if api_type == 'blog':
        url_template = "https://openapi.naver.com/v1/search/blog.json"
    elif api_type == 'news':
        url_template = "https://openapi.naver.com/v1/search/news.json"
    else:
        return pd.DataFrame() 

    # âš ï¸ [ìˆ˜ì •] ìµœì¢… DFëŠ” ì¼ê´€ëœ ì»¬ëŸ¼ëª…ì„ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.
    df_columns = ['title', 'link', 'description', 'channel_name', 'postdate', 'channel_type'] 
    df = pd.DataFrame(columns=df_columns)
    display = 100
    remove_tag = re.compile('<.*?>')

    for start in range(1, min(total_results, 1001) + 1, display): 
        # âš ï¸ [ìˆ˜ì •] queryë¥¼ URL ì¸ì½”ë”©í•˜ì§€ ì•Šì•„ë„ requests ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•´ì¤ë‹ˆë‹¤.
        url = url_template
        headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret
        }
        params = {
            "query": query,
            "display": display,
            "start": start
        }

        try:
            response = requests.get(url, headers=headers, params=params)
            response.raise_for_status()
            data = response.json()
            items = data.get("items", [])
            
            if not items:
                break

            processed_items = []
            for item in items:
                clean_item = {}
                clean_item['title'] = re.sub(remove_tag, '', item.get('title', ''))
                clean_item['description'] = re.sub(remove_tag, '', item.get('description', ''))
                clean_item['link'] = item.get('link', '') 
                
                # âš ï¸ [ìˆ˜ì •] ì±„ë„ëª… ë° ë‚ ì§œ ì²˜ë¦¬ ë¡œì§ í†µì¼
                if api_type == 'blog':
                    clean_item['channel_name'] = re.sub(remove_tag, '', item.get('bloggername', ''))
                    raw_date = item.get('postdate', '')
                    clean_item['postdate'] = pd.to_datetime(raw_date, format='%Y%m%d', errors='coerce').normalize() # YYYYMMDD í˜•ì‹
                elif api_type == 'news':
                    clean_item['channel_name'] = item.get('publisher', '') 
                    raw_date = item.get('pubDate', '')
                    # RFC 822 í˜•ì‹ ì²˜ë¦¬ (ì˜ˆ: Wed, 25 Oct 2023 00:00:00 +0900)
                    clean_item['postdate'] = pd.to_datetime(raw_date, format='%a, %d %b %Y %H:%M:%S +0900', errors='coerce').normalize()
                    
                clean_item['channel_type'] = api_type # ì±„ë„ íƒ€ì… ì¶”ê°€

                processed_items.append(clean_item)

            temp_df = pd.DataFrame(processed_items)
            df = pd.concat([df, temp_df], ignore_index=True)

        except requests.exceptions.RequestException as e:
            print(f"{api_type.capitalize()} API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (start={start}): {e}")
            break 

    df = df.dropna(subset=['postdate']).drop_duplicates()
    df = df[df['postdate'] <= datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)]
    print(f"-> ì´ {len(df)}ê±´ì˜ '{api_type.capitalize()}' ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    return df


def get_search_trend(query, client_id, client_secret):
    """ ë„¤ì´ë²„ ë°ì´í„°ë© APIë¥¼ ì´ìš©í•´ ìµœê·¼ 1ë…„ê°„ì˜ ì›”ê°„ ê²€ìƒ‰ëŸ‰ ì¶”ì´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. """
    # âš ï¸ [ìˆ˜ì •] ì£¼ê°„(90ì¼) ëŒ€ì‹  ì›”ê°„(365ì¼) íŠ¸ë Œë“œë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
    print(f"\n--- 2ë‹¨ê³„: '{query}' ì›”ê°„ ê²€ìƒ‰ëŸ‰ ì¶”ì´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ìµœê·¼ 1ë…„) ---")

    url = "https://openapi.naver.com/v1/datalab/search"
    end_date = datetime.now().strftime('%Y-%m-%d')
    start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d') # ìµœê·¼ 1ë…„

    body = json.dumps({
        "startDate": start_date,
        "endDate": end_date,
        "timeUnit": "month",  # âš ï¸ [ìˆ˜ì •] ì›”ê°„ ë‹¨ìœ„ ìš”ì²­
        "keywordGroups": [{"groupName": query, "keywords": [query]}]
    })
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
        "Content-Type": "application/json"
    }

    try:
        response = requests.post(url, headers=headers, data=body)
        response.raise_for_status()
        data = response.json()

        if not data.get('results') or not data['results'][0].get('data'):
             print("-> âŒ ë°ì´í„°ë© API ì‘ë‹µì— ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
             return pd.DataFrame(columns=['date', 'ratio']) # âš ï¸ [ìˆ˜ì •] ë°˜í™˜ ì»¬ëŸ¼ëª… í†µì¼

        trend_df = pd.DataFrame(data['results'][0]['data'])
        # âš ï¸ [ìˆ˜ì •] ì»¬ëŸ¼ëª…ì„ 'date', 'ratio'ë¡œ í†µì¼í•˜ì—¬ êµì°¨ ë¶„ì„ í•¨ìˆ˜ì—ì„œ ì‚¬ìš©í•˜ê¸° ì‰½ê²Œ í•©ë‹ˆë‹¤.
        trend_df = trend_df.rename(columns={'period': 'date', 'ratio': 'ratio'}) 
        
        trend_df['date'] = pd.to_datetime(trend_df['date']) 
        
        print(f"-> ìµœê·¼ 1ë…„ê°„ì˜ ì›”ê°„ ê²€ìƒ‰ëŸ‰ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤. (ì´ {len(trend_df)}ê±´)")
        return trend_df[['date', 'ratio']]

    except requests.exceptions.RequestException as e:
        print(f"ë°ì´í„°ë© API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame(columns=['date', 'ratio'])
    except Exception as e:
        print(f"ë°ì´í„°ë© ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame(columns=['date', 'ratio'])
    
# ----------------------------------------------------
# --- ì‹œê°í™” í•¨ìˆ˜ (í”Œë¡¯ ê°ì²´ ìƒì„±) ---
# ----------------------------------------------------

def visualize_post_frequency(df, frequency_type='monthly'): 
    """ ë‹¨ì¼ í†µí•© ë°ì´í„°í”„ë ˆì„ì„ ë°›ì•„ ì›”ë³„ ë˜ëŠ” ì£¼ë³„ ì–¸ê¸‰ëŸ‰ ì¶”ì´ë¥¼ ì‹œê°í™”í•˜ê³  í”Œë¡¯ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. """
    time_unit = "ì›”ë³„" if frequency_type == 'monthly' else "ì£¼ë³„"
    time_span = "12ê°œì›”" if frequency_type == 'monthly' else "6ê°œì›”"
    color = 'darkorange' if frequency_type == 'monthly' else 'purple'
    
    print(f"\n--- 3ë‹¨ê³„ ë¶„ì„: {time_unit} ì–¸ê¸‰ëŸ‰ ì‹œê°í™” ({time_span}) (í˜„ì¬ ê¸°ê°„ ì œì™¸) ---")
    
    temp_df = df.copy()
    
    try:
        temp_df['postdate'] = pd.to_datetime(temp_df['postdate'], errors='coerce').dt.normalize()
    except Exception:
        print(f"ê²½ê³ : ë‚ ì§œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ë¶„ì„ì„ ê±´ë„ˆëœ•ë‹ˆë‹¤.")
        return None  # None ë°˜í™˜ìœ¼ë¡œ í†µì¼
    
    temp_df.dropna(subset=['postdate'], inplace=True)
    if temp_df.empty:
        print("ê²½ê³ : ìœ íš¨í•œ ë‚ ì§œë¥¼ ê°€ì§„ ê²Œì‹œë¬¼ ë°ì´í„°ê°€ ì—†ì–´ ë¶„ì„ì„ ê±´ë„ˆëœ•ë‹ˆë‹¤.")
        return None
        
    now = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    
    # ğŸŒŸ ì´ì „ì— ìƒëµ ì£¼ì„ì´ ìˆë˜ ë¶€ë¶„ (ì´ì œ ë¡œì§ì´ ëª¨ë‘ í¬í•¨ë˜ì–´ ê¹”ë”í•´ì¡ŒìŠµë‹ˆë‹¤) ğŸŒŸ
    if frequency_type == 'monthly':
        current_month_start = now.replace(day=1)
        # í˜„ì¬ ì›”ë³´ë‹¤ ì´ì „ ë°ì´í„°ë§Œ ì‚¬ìš©
        df_filtered = temp_df[temp_df['postdate'] < current_month_start].copy()
        start_date_offset = pd.DateOffset(months=12)
        freq_label = 'post_month'
        freq_unit = 'MS'
        
        if not df_filtered.empty:
            # ìµœê·¼ 12ê°œì›” ë°ì´í„°ë§Œ í•„í„°ë§ (ìµœì‹  ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ 12ê°œì›”)
            latest_date = df_filtered['postdate'].max()
            df_filtered = df_filtered[df_filtered['postdate'] >= (latest_date - start_date_offset)].copy()
            df_filtered[freq_label] = df_filtered['postdate'].dt.strftime('%Y-%m')
            counts_raw = df_filtered[freq_label].value_counts().sort_index()
            
            # ë¹ˆ ì›”ì„ ì±„ìš°ê¸° ìœ„í•œ ì „ì²´ ë ˆì´ë¸” ìƒì„±
            start_month = counts_raw.index.min() if not counts_raw.empty else (now - start_date_offset).strftime('%Y-%m')
            end_month = counts_raw.index.max() if not counts_raw.empty else (now - pd.DateOffset(months=1)).strftime('%Y-%m')
            
            full_index_range = pd.date_range(start=start_month, end=end_month, freq=freq_unit)
            full_labels = full_index_range.strftime('%Y-%m')
            rotation_angle = 45
        else:
            print(f"ê²½ê³ : í•„í„°ë§ ê²°ê³¼ ìµœê·¼ {time_span} ì´ë‚´ì˜ ìœ íš¨í•œ ê²Œì‹œë¬¼ì´ ì—†ì–´ {time_unit} ë¶„ì„ì„ ê±´ë„ˆëœ•ë‹ˆë‹¤.")
            return None # None ë°˜í™˜ìœ¼ë¡œ í†µì¼

    elif frequency_type == 'weekly':
        # í˜„ì¬ ì£¼ì°¨ì˜ ì‹œì‘ì¼ (ì›”ìš”ì¼ ì‹œì‘ìœ¼ë¡œ ê°€ì •)
        # í˜„ì¬ ì£¼ì°¨ ì‹œì‘ì¼ (ì›”ìš”ì¼)
        current_week_start = (now - timedelta(days=now.weekday()))
        
        # í˜„ì¬ ì£¼ë³´ë‹¤ ì´ì „ ë°ì´í„°ë§Œ ì‚¬ìš©
        df_filtered = temp_df[temp_df['postdate'] < current_week_start].copy()
        start_date_offset = pd.DateOffset(months=6) # ì£¼ë³„ì€ ìµœê·¼ 6ê°œì›” ê¸°ì¤€

        if not df_filtered.empty:
            # ìµœê·¼ 6ê°œì›” ë°ì´í„°ë§Œ í•„í„°ë§
            latest_date = df_filtered['postdate'].max()
            df_filtered = df_filtered[df_filtered['postdate'] >= (latest_date - start_date_offset)].copy()
            
            # ** ğŸ¯ í•µì‹¬ ìˆ˜ì •: resampleì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì°¨ë³„ ì¹´ìš´íŠ¸ ë° ë¹ˆ ì£¼ì°¨ ì±„ìš°ê¸° **
            
            # postdateë¥¼ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
            df_resample = df_filtered.set_index('postdate')
            
            # 'W'ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ë³„ë¡œ ì¬í‘œë³¸ ì¶”ì¶œ (ê¸°ë³¸ì ìœ¼ë¡œ ì¼ìš”ì¼ì— ëë‚˜ëŠ” ì£¼ì°¨ë¡œ ì¹´ìš´íŠ¸)
            counts_resampled = df_resample.resample('W').size() # count.value_counts() ëŒ€ì‹  size() ì‚¬ìš©
            
            full_counts = counts_resampled
            
            # resample ê²°ê³¼ ì¸ë±ìŠ¤ (ë‚ ì§œ ê°ì²´)ë¥¼ 'YYYY-MM-DD' í˜•ì‹ì˜ ë¬¸ìì—´ ë ˆì´ë¸”ë¡œ ë³€í™˜
            full_counts.index = full_counts.index.strftime('%Y-%m-%d')
            full_labels = full_counts.index.tolist() # ì¬ìƒ‰ì¸ ë¶ˆí•„ìš”
            rotation_angle = 90
        else:
            print(f"ê²½ê³ : í•„í„°ë§ ê²°ê³¼ ìµœê·¼ {time_span} ì´ë‚´ì˜ ìœ íš¨í•œ ê²Œì‹œë¬¼ì´ ì—†ì–´ {time_unit} ë¶„ì„ì„ ê±´ë„ˆëœ•ë‹ˆë‹¤.")
            return None # None ë°˜í™˜ìœ¼ë¡œ í†µì¼
    
    else:
        return None

    if 'full_counts' not in locals() or full_counts.empty: # full_counts ë³€ìˆ˜ ìƒì„± í™•ì¸ ë° ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        print(f"ê²½ê³ : í•„í„°ë§ ê²°ê³¼ ìµœê·¼ {time_span} ì´ë‚´ì˜ ìœ íš¨í•œ ê²Œì‹œë¬¼ì´ ì—†ì–´ {time_unit} ë¶„ì„ì„ ê±´ë„ˆëœ•ë‹ˆë‹¤.")
        return None

    # ì›”ë³„ ë¡œì§ì„ ë”°ë¥´ê¸° ìœ„í•´ full_countsë¥¼ counts_rawì™€ full_labelsë¡œ ë¶„ë¦¬ (ì£¼ë³„ ë¡œì§ì€ ì´ë¯¸ full_countsì— í†µí•©ë¨)
    if frequency_type == 'monthly':
        counts_series = counts_raw.rename('count')
        full_counts = counts_series.reindex(full_labels, fill_value=0)
    
    # ì‹œê°í™” ì‹¤í–‰
    plt.figure(figsize=(15 if frequency_type == 'weekly' else 12, 6))
    sns.lineplot(
        x=full_counts.index, y=full_counts.values, marker='o', color=color
    )

    plt.title(f'ìµœê·¼ {time_span} ì–¸ê¸‰ëŸ‰ ì¶”ì´ (í˜„ì¬ {time_unit[:-1]} ì œì™¸)', fontsize=16) 
    
    # ì£¼ë³„/ì›”ë³„ì— ë”°ë¼ xì¶• ë ˆì´ë¸” ì„¤ì •
    if frequency_type == 'weekly':
        plt.xlabel(f'ì–¸ê¸‰ {time_unit} (ì£¼ì°¨ ì¢…ë£Œì¼)', fontsize=12) # ë ˆì´ë¸” ìˆ˜ì •
    else:
        plt.xlabel(f'ì–¸ê¸‰ {time_unit}', fontsize=12) 
        
    plt.ylabel('ì´ ì–¸ê¸‰ëŸ‰', fontsize=12)

    plt.xticks(full_counts.index, rotation=rotation_angle) 
    plt.grid(axis='y', linestyle='--')
    plt.tight_layout()
    
    print(f"-> í†µí•© ë°ì´í„° {time_unit} ì–¸ê¸‰ëŸ‰ ì‹œê°í™” í”Œë¡¯ ê°ì²´ ìƒì„± ì™„ë£Œ.")
    return plt

def visualize_combined_trend(total_df, trend_df):
    """ ê²€ìƒ‰ëŸ‰(ì›”ê°„)ê³¼ ì–¸ê¸‰ëŸ‰(ì›”ê°„)ì„ í•˜ë‚˜ì˜ ê·¸ë˜í”„ì— ì‹œê°í™”í•˜ê³  í”Œë¡¯ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. """
    print("\n--- 4ë‹¨ê³„ ë¶„ì„: ì–¸ê¸‰ëŸ‰ vs ê²€ìƒ‰ëŸ‰ í†µí•© ì‹œê°í™” (ìµœê·¼ 1ë…„ ì›”ê°„ ë‹¨ìœ„) ---")
    
    if trend_df.empty or 'date' not in trend_df.columns:
        print("ê²½ê³ : ê²€ìƒ‰ëŸ‰(ë°ì´í„°ë©) ë°ì´í„°ê°€ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜ëª»ë˜ì–´ í†µí•© ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
        
    temp_df = total_df.copy()
    
    try:
        temp_df['postdate'] = pd.to_datetime(temp_df['postdate'], errors='coerce').dt.normalize()
        temp_df.dropna(subset=['postdate'], inplace=True)
    except Exception:
        return

    temp_df['month_start_date'] = temp_df['postdate'].dt.to_period('M').apply(lambda x: x.start_time).dt.normalize() 
    mention_counts = temp_df['month_start_date'].value_counts().reset_index()
    mention_counts.columns = ['date', 'mention_count']
    
    trend_df['date'] = trend_df['date'].dt.to_period('M').apply(lambda x: x.start_time).dt.normalize()
    
    start_date = trend_df['date'].min()
    end_date = trend_df['date'].max()
    mention_counts = mention_counts[
        (mention_counts['date'] >= start_date) & (mention_counts['date'] <= end_date)
    ]
    
    combined_df = pd.merge(trend_df, mention_counts, on='date', how='left').fillna(0)
    
    max_mention = combined_df['mention_count'].max()
    combined_df['mention_ratio'] = (combined_df['mention_count'] / max_mention) * 100 if max_mention > 0 else 0.0
    
    # ì‹œê°í™” (Dual Axis)
    fig, ax1 = plt.subplots(figsize=(12, 6))
    x_labels = combined_df['date'].dt.strftime('%Y-%m')

    color = 'tab:red'
    ax1.set_xlabel('ì›”ê°„ (ì‹œì‘ì¼ ê¸°ì¤€)', fontsize=12)
    ax1.set_ylabel('ê²€ìƒ‰ëŸ‰ ë¹„ìœ¨ (0-100) - Datalab ê¸°ì¤€', color=color, fontsize=12)
    sns.lineplot(x=combined_df.index, y='ratio', data=combined_df, marker='o', color=color, ax=ax1, label='ê²€ìƒ‰ëŸ‰')
    ax1.tick_params(axis='y', labelcolor=color)
    ax1.set_ylim(0, 100)
    ax1.set_xticks(combined_df.index)
    ax1.set_xticklabels(x_labels, rotation=45, ha='right')
    ax1.legend(loc='upper left')

    ax2 = ax1.twinx() 
    color = 'tab:blue'
    ax2.set_ylabel('ì–¸ê¸‰ëŸ‰ ë¹„ìœ¨ (0-100) - ìì²´ ìµœëŒ€ ì–¸ê¸‰ëŸ‰ ê¸°ì¤€', color=color, fontsize=12) 
    sns.lineplot(x=combined_df.index, y='mention_ratio', data=combined_df, marker='s', color=color, ax=ax2, label='ì–¸ê¸‰ëŸ‰')
    ax2.tick_params(axis='y', labelcolor=color)
    ax2.set_ylim(0, 100)
    ax2.legend(loc='upper right')
    
    plt.title('ìµœê·¼ 1ë…„ ì›”ê°„ ê²€ìƒ‰ëŸ‰ ë° ì–¸ê¸‰ëŸ‰ ì¶”ì´ ë¹„êµ (0-100 ë¹„ìœ¨)', fontsize=16)
    fig.tight_layout()
    print("-> ê²€ìƒ‰ëŸ‰ ë° ì–¸ê¸‰ëŸ‰ í†µí•© ì‹œê°í™” í”Œë¡¯ ê°ì²´ ìƒì„± ì™„ë£Œ.")
    return plt

def visualize_sentiment_word_clouds(df, positive_words, negative_words):
    """ ê°ì„± ì‚¬ì „ ê¸°ë°˜ ê¸ì •/ë¶€ì • ì›Œë“œ í´ë¼ìš°ë“œ í”Œë¡¯ ê°ì²´ì™€ ìƒìœ„ 7ê°œ í‚¤ì›Œë“œ DFë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. """
    print("\n--- 5ë‹¨ê³„ ë¶„ì„: ê°ì„± ì‚¬ì „ ê¸°ë°˜ ê¸ì •/ë¶€ì • ë° í†µí•© í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘ ---")
    
    if df.empty or 'description' not in df.columns:
        return (None, None, None, pd.DataFrame())

    text = ' '.join(df['description'].astype(str).tolist())
    cleaned_text = re.sub('[^ê°€-í£\s]', '', text).lower()
    
    def get_word_counts(word_list, text_corpus, max_words=20):
        counts = {}
        for word in word_list:
            count = len(re.findall(r'\b' + re.escape(word) + r'\b', text_corpus)) 
            if count > 0:
                counts[word] = count
        return dict(sorted(counts.items(), key=lambda item: item[1], reverse=True)[:max_words])
    
    def create_wordcloud_plot(counts, title, colormap):
        if not counts:
            return None

        wc = WordCloud(
            font_path=FONT_PATH, width=800, height=400, background_color='white',
            max_words=20, colormap=colormap, prefer_horizontal=0.9, collocations=False 
        )
        wordcloud = wc.generate_from_frequencies(counts)
        
        fig = plt.figure(figsize=(12, 7)) 
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off') 
        plt.title(title, fontsize=16)
        plt.tight_layout(pad=3.0) 
        
        return fig

    pos_counts = get_word_counts(positive_words, cleaned_text, max_words=20)
    pos_plot = create_wordcloud_plot(pos_counts, 'ê¸ì • ê°ì„± í‚¤ì›Œë“œ íƒœê·¸ í´ë¼ìš°ë“œ (Top 20)', 'YlGn')
    
    neg_counts = get_word_counts(negative_words, cleaned_text, max_words=20)
    neg_plot = create_wordcloud_plot(neg_counts, 'ë¶€ì • ê°ì„± í‚¤ì›Œë“œ íƒœê·¸ í´ë¼ìš°ë“œ (Top 20)', 'Reds_r')

    ALL_SENTIMENT_WORDS = positive_words + negative_words
    all_counts_top20 = get_word_counts(ALL_SENTIMENT_WORDS, cleaned_text, max_words=20)
    all_plot = create_wordcloud_plot(all_counts_top20, 'ê¸ì •+ë¶€ì • í†µí•© í‚¤ì›Œë“œ íƒœê·¸ í´ë¼ìš°ë“œ (Top 20)', 'plasma')
    
    all_counts_top7 = {k: all_counts_top20[k] for k in list(all_counts_top20.keys())[:min(7, len(all_counts_top20))]}
    all_df = pd.DataFrame(all_counts_top7.items(), columns=['í‚¤ì›Œë“œ', 'ì–¸ê¸‰ íšŸìˆ˜'])
    
    print("-> ê°ì„± ì‚¬ì „ ê¸°ë°˜ ì›Œë“œí´ë¼ìš°ë“œ í”Œë¡¯ ê°ì²´ ìƒì„± ë° ìƒìœ„ í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ.")
    return pos_plot, neg_plot, all_plot, all_df

def visualize_competitor_mention_comparison(own_query, own_df, competitor_query, competitor_df):
    """ ìì‚¬/ê²½ìŸì‚¬ ì›”ë³„ ì–¸ê¸‰ëŸ‰ì„ ë¹„êµí•˜ì—¬ ì‹œê°í™”í•˜ê³  í”Œë¡¯ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. """
    print(f"\n--- 9ë‹¨ê³„ ë¶„ì„: ìì‚¬({own_query}) vs ê²½ìŸì‚¬({competitor_query}) ì›”ë³„ ì–¸ê¸‰ëŸ‰ ë¹„êµ ì‹œê°í™” ì‹œì‘ ---")

    if own_df.empty and competitor_df.empty:
        print("ê²½ê³ : ìì‚¬ì™€ ê²½ìŸì‚¬ ëª¨ë‘ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ ë¹„êµ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    def prepare_monthly_counts(df, label):
        temp_df = df.copy()
        if temp_df.empty: return pd.Series([], dtype='int64', name=label)
        try:
            temp_df['postdate'] = pd.to_datetime(temp_df['postdate'], errors='coerce').dt.normalize()
            temp_df.dropna(subset=['postdate'], inplace=True)
            
            now = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
            current_month_start = now.replace(day=1)
            start_date_offset = pd.DateOffset(months=12)
            
            df_filtered = temp_df[temp_df['postdate'] < current_month_start].copy()
            if df_filtered.empty: return pd.Series([], dtype='int64', name=label)
                 
            latest_date = df_filtered['postdate'].max()
            df_filtered = df_filtered[df_filtered['postdate'] >= (latest_date - start_date_offset)].copy()
            
            df_filtered['post_month'] = df_filtered['postdate'].dt.strftime('%Y-%m')
            counts = df_filtered['post_month'].value_counts().sort_index().rename(label)
            return counts
        except Exception as e:
            print(f"ê²½ê³ : {label} ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. {e}")
            return pd.Series([], dtype='int64', name=label)

    own_counts = prepare_monthly_counts(own_df, own_query)
    comp_counts = prepare_monthly_counts(competitor_df, competitor_query)
    combined_counts = pd.concat([own_counts, comp_counts], axis=1).fillna(0)
    combined_counts.index.name = 'Month'
    
    if combined_counts.shape[0] < 2:
        print("ê²½ê³ : ë¹„êµí•  ì›” ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. (ìµœì†Œ 2ê°œì›” í•„ìš”)")
        return None

    plt.figure(figsize=(14, 7))
    sns.lineplot(x=combined_counts.index, y=own_query, data=combined_counts, marker='o', color='tab:blue', label=own_query)
    sns.lineplot(x=combined_counts.index, y=competitor_query, data=combined_counts, marker='s', color='tab:red', label=competitor_query)

    plt.title(f'{own_query} vs {competitor_query} ì›”ë³„ ì–¸ê¸‰ëŸ‰ ì¶”ì´ ë¹„êµ (ìµœê·¼ 12ê°œì›”)', fontsize=16) 
    plt.xlabel('ì›”', fontsize=12) 
    plt.ylabel('ì´ ì–¸ê¸‰ëŸ‰', fontsize=12)
    plt.xticks(combined_counts.index, rotation=45) 
    plt.legend(fontsize=12)
    plt.grid(axis='y', linestyle='--')
    plt.tight_layout()
    
    print("-> ìì‚¬/ê²½ìŸì‚¬ ì–¸ê¸‰ëŸ‰ ë¹„êµ ì‹œê°í™” í”Œë¡¯ ê°ì²´ ìƒì„± ì™„ë£Œ.")
    return plt


# ----------------------------------------------------
# --- í•µì‹¬ ë¶„ì„ í•¨ìˆ˜ (ì§€í‘œ ê³„ì‚° ë° ê°ì„± ë¶„ì„) ---
# ----------------------------------------------------

def find_outbreak_weeks(trend_df, change_threshold=0.5):
    """ ì›”ê°„ ê²€ìƒ‰ëŸ‰ ë¹„ìœ¨ì„ ê¸°ì¤€ìœ¼ë¡œ ì „ì›” ëŒ€ë¹„ ê²€ìƒ‰ëŸ‰ì´ ê¸‰ì¦í•œ ì›”ê°„ì„ ì°¾ìŠµë‹ˆë‹¤. """
    print(f"\n--- 6ë‹¨ê³„ ë¶„ì„: ì´ìŠˆ í™•ì‚° ì›”ê°„ ì¶”ì¶œ ì‹œì‘ (ì „ì›” ëŒ€ë¹„ {change_threshold * 100:.0f}% ì´ˆê³¼ ì¦ê°€ ê¸°ì¤€) ---")
    
    if trend_df.empty or 'date' not in trend_df.columns:
        print("-> âŒ ê²€ìƒ‰ëŸ‰(ë°ì´í„°ë©) ë°ì´í„°ê°€ ì—†ì–´ í™•ì‚° ì›”ê°„ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    now = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0).replace(day=1)
    filtered_trend_df = trend_df[trend_df['date'] < now].copy()
    filtered_trend_df = filtered_trend_df.sort_values(by='date')
    
    if filtered_trend_df.empty:
        print("-> âŒ ìœ íš¨í•œ ê³¼ê±° ì›”ê°„ ë°ì´í„°ê°€ ì—†ì–´ í™•ì‚° ì›”ê°„ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    filtered_trend_df['ratio'] = pd.to_numeric(filtered_trend_df['ratio'], errors='coerce')
    filtered_trend_df.dropna(subset=['ratio'], inplace=True)
    filtered_trend_df['prev_ratio'] = filtered_trend_df['ratio'].shift(1).fillna(0)
    
    filtered_trend_df['change_rate'] = filtered_trend_df.apply(
        lambda row: (row['ratio'] - row['prev_ratio']) / row['prev_ratio'] 
                     if row['prev_ratio'] > 0 else (100.0 if row['ratio'] > 0 else 0), 
        axis=1
    )
    
    outbreak_months_df = filtered_trend_df[
        ((filtered_trend_df['prev_ratio'] > 0) & (filtered_trend_df['change_rate'] > change_threshold)) | 
        ((filtered_trend_df['prev_ratio'] == 0) & (filtered_trend_df['ratio'] > 0))
    ].copy()
    
    outbreak_results = []
    if not outbreak_months_df.empty:
        outbreak_months_df = outbreak_months_df.sort_values(by='ratio', ascending=False)
        for _, row in outbreak_months_df.iterrows():
            month_start = row['date'].strftime('%Y-%m')
            current_ratio = row['ratio']
            prev_ratio = row['prev_ratio']
            
            rate_str = f"{row['change_rate'] * 100:.1f}% ì¦ê°€" if row['prev_ratio'] > 0 else "ì‹ ê·œ ë°œìƒ (ì „ì›” 0)"
            outbreak_results.append(f"{month_start} (í˜„ì¬ ë¹„ìœ¨: {current_ratio:.1f}, ì „ì›”: {prev_ratio:.1f}, {rate_str})")
            
        print(f"-> âœ… ì´ {len(outbreak_results)}ê°œì˜ ê²€ìƒ‰ëŸ‰ ê¸‰ì¦ ì›”ê°„ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
    return outbreak_results

def calculate_lexicon_score(text_corpus):
    """ ê°ì„± ì‚¬ì „ì„ ê¸°ë°˜ìœ¼ë¡œ ê¸ì •ì„± ì§€ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. """
    print("\n--- 7ë‹¨ê³„ ë¶„ì„: ìˆœìˆ˜ Python ê°ì„± ì‚¬ì „ì„ ì´ìš©í•œ ê°ì„± ë¶„ì„ ì‹œì‘ ---")
    
    cleaned_text = re.sub('[^ê°€-í£\s]', '', text_corpus).lower()
    pos_count = sum(len(re.findall(r'\b' + re.escape(word) + r'\b', cleaned_text)) for word in POSITIVE_WORDS)
    neg_count = sum(len(re.findall(r'\b' + re.escape(word) + r'\b', cleaned_text)) for word in NEGATIVE_WORDS)

    total_sentiment_count = pos_count + neg_count

    if total_sentiment_count == 0:
        print("-> ê°ì„± ë‹¨ì–´ê°€ í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ë˜ì§€ ì•Šì•„ ì¤‘ë¦½ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
        return 50.0

    positive_score = (pos_count / total_sentiment_count) * 100
    
    print(f"-> ê¸ì • ë‹¨ì–´: {pos_count}íšŒ, ë¶€ì • ë‹¨ì–´: {neg_count}íšŒ")
    print(f"-> ê³„ì‚°ëœ ê¸ì •ì„± ì§€ìˆ˜: {positive_score:.2f}%")
    
    return positive_score

def classify_sentiment(positive_score):
    """ ì‚¬ìš©ì ì§€ì • ê·œì¹™ì— ë”°ë¼ ìµœì¢… ê°ì„±ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤. """
    if positive_score > 60:
        return f"ğŸŸ¢ ê¸ì • ({positive_score:.1f}%)"
    else: 
        return f"ğŸ”´ ë¶€ì • ({positive_score:.1f}%)"

def calculate_key_metrics(df):
    """ ìµœë‹¤ ì–¸ê¸‰ëŸ‰ ë‚ ì§œì™€ ì–¸ê¸‰ëŸ‰ ì¦ê°ë¥ (ìµœê·¼ 30ì¼ vs ì´ì „ 30ì¼)ì„ ê³„ì‚°í•©ë‹ˆë‹¤. """
    default_date = 'N/A'
    default_rate = 'N/A'
    
    temp_df = df.copy()
    print("\n--- 8ë‹¨ê³„ ë¶„ì„: ìµœë‹¤ ì–¸ê¸‰ ë‚ ì§œ ë° ì¦ê°ë¥  ê³„ì‚° ì‹œì‘ ---")
    
    try:
        temp_df['postdate'] = pd.to_datetime(temp_df['postdate'], errors='coerce').dt.normalize()
        temp_df.dropna(subset=['postdate'], inplace=True)
    except Exception as e:
        print(f"-> âŒ ì–¸ê¸‰ëŸ‰ ë¶„ì„: ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨. {e}")
        return default_date, default_rate

    if temp_df.empty: return default_date, default_rate

    # 1. ìµœë‹¤ ì–¸ê¸‰ëŸ‰ ë‚ ì§œ
    daily_counts = temp_df['postdate'].dt.normalize().value_counts()
    most_frequent_date = default_date
    if not daily_counts.empty:
        max_count = daily_counts.max()
        most_frequent_date = daily_counts[daily_counts == max_count].index.max().strftime('%Y-%m-%d')
        print(f"-> ìµœë‹¤ ì–¸ê¸‰ëŸ‰ ë‚ ì§œ: {most_frequent_date} ({max_count}íšŒ)")

    # 2. ì–¸ê¸‰ëŸ‰ ì¦ê°ë¥ 
    now = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    start_d1, end_d1 = now - timedelta(days=30), now 
    start_d2, end_d2 = now - timedelta(days=60), now - timedelta(days=30)

    mentions_d1 = temp_df[(temp_df['postdate'].dt.normalize() >= start_d1) & (temp_df['postdate'].dt.normalize() < end_d1)].shape[0]
    mentions_d2 = temp_df[(temp_df['postdate'].dt.normalize() >= start_d2) & (temp_df['postdate'].dt.normalize() < end_d2)].shape[0]

    mention_change_rate = default_rate

    if mentions_d2 > 0:
        change_rate = ((mentions_d1 - mentions_d2) / mentions_d2) * 100
        mention_change_rate = f"{change_rate:+.1f}%" 
    elif mentions_d1 > 0:
        mention_change_rate = "+100% (ì‹ ê·œ)"
    else:
        mention_change_rate = "0.0%"

    print(f"-> ì¦ê°ë¥  ë¶„ì„: ìµœê·¼ 30ì¼({mentions_d1}ê±´) vs ì´ì „ 30ì¼({mentions_d2}ê±´). ì¦ê°ë¥ : {mention_change_rate}")
    return most_frequent_date, mention_change_rate


# ----------------------------------------------------
# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (Flask API ì—”ë“œí¬ì¸íŠ¸ì— ì‚¬ìš©) ---
# ----------------------------------------------------

def run_full_analysis(search_query: str, competitor_query: str, client_id: str, client_secret: str, max_results: int, static_folder: str) -> dict:
    """
    ë¸Œëœë“œ í‰íŒ ë¶„ì„ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ JSON ì‘ë‹µìš© ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    
    analysis_results = {
        "query": search_query,
        "competitor_query": competitor_query,
        "status": "FAILURE",
        "message": "ë¶„ì„ ì‹¤íŒ¨: ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜",
        "key_metrics": {},
        "sentiment_analysis": {},
        "trend_analysis": {},
        "visualization_urls": {}
    }

    # 1. ìœ íš¨ì„± ê²€ì¦
    if not is_valid_query(search_query) or not is_brand_name(search_query, client_id, client_secret):
        analysis_results["message"] = "ìì‚¬ ê²€ìƒ‰ì–´ ìœ íš¨ì„±/ë¸Œëœë“œëª… ê²€ì¦ì— ì‹¤íŒ¨í•˜ì—¬ ë¶„ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."
        return analysis_results
    
    is_comp_valid = competitor_query and is_valid_query(competitor_query)
    if not is_comp_valid: competitor_query = None

    print("\n==================================================")
    print("âœ… ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("==================================================")
    
    # 2. ìì‚¬ ë°ì´í„° ìˆ˜ì§‘ ë° í†µí•©
    blog_df = fetch_naver_search_results(search_query, 'blog', client_id, client_secret, max_results)
    news_df = fetch_naver_search_results(search_query, 'news', client_id, client_secret, max_results)
    total_df = pd.concat([blog_df, news_df], ignore_index=True)
    
    # 3. ê²€ìƒ‰ëŸ‰ ì¶”ì´ ë¶„ì„ (ì›”ê°„)
    trend_df = get_search_trend(search_query, client_id, client_secret)
    
    # 4. ê²½ìŸì‚¬ ë°ì´í„° ìˆ˜ì§‘
    competitor_df = pd.DataFrame()
    if is_comp_valid and competitor_query:
        comp_blog_df = fetch_naver_search_results(competitor_query, 'blog', client_id, client_secret, max_results)
        comp_news_df = fetch_naver_search_results(competitor_query, 'news', client_id, client_secret, max_results)
        competitor_df = pd.concat([comp_blog_df, comp_news_df], ignore_index=True)

    if total_df.empty:
        analysis_results["message"] = "ìˆ˜ì§‘ëœ ìì‚¬ ë¸”ë¡œê·¸/ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ì–´ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤."
        analysis_results["status"] = "INSUFFICIENT_DATA"
        return analysis_results

    # 5. ë¶„ì„ ë° ì§€í‘œ ê³„ì‚°
    total_description = ' '.join(total_df['description'].astype(str).tolist())
    positive_score = calculate_lexicon_score(total_description) 
    final_sentiment = classify_sentiment(positive_score)
    most_frequent_date_result, mention_change_rate_result = calculate_key_metrics(total_df)
    initial_outbreak_months = find_outbreak_weeks(trend_df, change_threshold=0.5) 
    
    # 6. ì‹œê°í™” ë° URL ì €ì¥
    gc.collect() 
    
    # âš ï¸ [ìˆ˜ì •] ëˆ„ë½ëœ ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ ë° URL ì €ì¥ ë¡œì§ ì¶”ê°€
    urls = {}
    
    # 6-1. ê°ì„± ì›Œë“œí´ë¼ìš°ë“œ (top7_keywords_df ë³€ìˆ˜ í™•ë³´)
    pos_plot, neg_plot, all_plot, top7_keywords_df = visualize_sentiment_word_clouds(
        total_df, POSITIVE_WORDS, NEGATIVE_WORDS
    )
    
    urls["positive_wordcloud"] = save_and_get_url(
        lambda: pos_plot, "sentiment_positive_wc.png", static_folder
    )
    urls["negative_wordcloud"] = save_and_get_url(
        lambda: neg_plot, "sentiment_negative_wc.png", static_folder
    )
    urls["combined_wordcloud"] = save_and_get_url(
        lambda: all_plot, "sentiment_combined_wc.png", static_folder
    )
    
    # 6-2. ì›”ë³„ ì–¸ê¸‰ëŸ‰ ì¶”ì´
    urls["monthly_frequency"] = save_and_get_url(
        lambda: visualize_post_frequency(total_df, frequency_type='monthly'),
        "mention_monthly_freq.png", static_folder
    )
    
    # 6-3. ì£¼ë³„ ì–¸ê¸‰ëŸ‰ ì¶”ì´ (ìµœê·¼ 6ê°œì›”)
    urls["weekly_frequency"] = save_and_get_url(
        lambda: visualize_post_frequency(total_df, frequency_type='weekly'),
        "mention_weekly_freq.png", static_folder
    )
    
    # 6-4. ê²€ìƒ‰ëŸ‰ vs ì–¸ê¸‰ëŸ‰ í†µí•©
    urls["combined_trend"] = save_and_get_url(
        lambda: visualize_combined_trend(total_df, trend_df),
        "mention_vs_search_trend.png", static_folder
    )
    
    # 6-5. ê²½ìŸì‚¬ ë¹„êµ
    if is_comp_valid and not competitor_df.empty:
        urls["competitor_comparison"] = save_and_get_url(
            lambda: visualize_competitor_mention_comparison(search_query, total_df, competitor_query, competitor_df),
            "competitor_comparison.png", static_folder
        )
    else:
        urls["competitor_comparison"] = None
    
    # 7. ìµœì¢… ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ êµ¬ì„± (í…œí”Œë¦¿ ì—°ë™ì„ ìœ„í•´ í‚¤ êµ¬ì¡° í‰íƒ„í™”)
    analysis_results = {} # ë¶„ì„ ê²°ê³¼ë¥¼ ìƒˆë¡œìš´ ë”•ì…”ë„ˆë¦¬ë¡œ í‰íƒ„í™”í•˜ì—¬ êµ¬ì„±

    # ê¸°ë³¸ ì •ë³´
    analysis_results["query"] = search_query
    analysis_results["competitor_query"] = competitor_query
    analysis_results["status"] = "SUCCESS"
    analysis_results["message"] = f"'{search_query}' ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."

    # í•µì‹¬ ì§€í‘œ (Key Metrics)
    analysis_results["total_mentions"] = len(total_df)
    analysis_results["most_frequent_date"] = most_frequent_date_result
    analysis_results["mention_change_rate"] = mention_change_rate_result
    analysis_results["competitor_mentions"] = len(competitor_df) if competitor_query else 0

    # ê°ì„± ë¶„ì„ (Sentiment Analysis)
    # í…œí”Œë¦¿ì˜ 'label'ê³¼ 'slider'ì— ë°”ë¡œ ì‚¬ìš©í•˜ê¸° ì‰½ê²Œ ë³€í™˜
    analysis_results["final_sentiment_label"] = final_sentiment 
    analysis_results["positive_percentage"] = int(float(f"{positive_score:.2f}")) 
    analysis_results["top_keywords"] = top7_keywords_df.to_dict('records') if not top7_keywords_df.empty else []

    # íŠ¸ë Œë“œ ë¶„ì„
    analysis_results["outbreak_weeks"] = initial_outbreak_months # ì´ìŠˆ í™•ì‚° í¬ì¸íŠ¸
    analysis_results["trend_data_available"] = not trend_df.empty

    # ì‹œê°í™” URL
    
    
    analysis_results["visualization_urls"] = urls


    # ğŸš€ [ì¶”ê°€ëœ ë¶€ë¶„] ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸ (ì œëª©, ë‚ ì§œ, ì‘ì„±ì, ë§í¬) ì¶”ì¶œ
    # total_dfì—ì„œ í•„ìš”í•œ 4ê°œ ì—´ë§Œ ì¶”ì¶œí•˜ì—¬ í…œí”Œë¦¿ì— ì „ë‹¬í•©ë‹ˆë‹¤.
    analysis_results["post_list"] = total_df[[
        'title', 
        'postdate', 
        'channel_name', 
        'link'
    ]].rename(columns={
        'postdate': 'date', 
        'channel_name': 'author'
    }).to_dict('records')

    # ğŸš€ [ì¶”ê°€ëœ ë¶€ë¶„] AI ë¦¬í¬íŠ¸ ê³µê°„ (ì¼ë‹¨ ë¹ˆ ë¬¸ìì—´ë¡œ í• ë‹¹)
    # ì¶”í›„ LLM ë“±ì„ í™œìš©í•˜ì—¬ ë‚´ìš©ì„ ì±„ìš¸ ìˆ˜ ìˆë„ë¡ í‚¤ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    analysis_results["ai_report"] = "AI ë¦¬í¬íŠ¸ ë‚´ìš©ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ í™•ì¸í•´ ì£¼ì„¸ìš”." 


    print("\n==================================================")
    print("âœ… ìµœì¢… ë¶„ì„ ê²°ê³¼ JSON ìƒì„± ì™„ë£Œ. Flask ì‘ë‹µ ì¤€ë¹„.")
    print("==================================================")

    return analysis_results

# ----------------------------------------------------
# --- ì½˜ì†” í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰ ë¸”ë¡ ---
# ----------------------------------------------------

if __name__ == "__main__":
    print("ì´ ì½”ë“œëŠ” Flask ì•±ì˜ ë°±ì—”ë“œ ëª¨ë“ˆë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ì‹¤ì œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œëŠ” Flask í™˜ê²½ê³¼ API í‚¤, í°íŠ¸ ê²½ë¡œ ë“±ì´ í•„ìš”í•©ë‹ˆë‹¤.")
