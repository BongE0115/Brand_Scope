import os
import pandas as pd
import json
import re
import requests 
from datetime import datetime, timedelta
from collections import Counter 
import gc 
import matplotlib.pyplot as plt 
import matplotlib
import requests.utils
import threading
import time
import markdown

# âš ï¸ [ì¶”ê°€] Flask ì—°ë™ ë° ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•´ í•„ìˆ˜ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import io 
import numpy as np 

# íƒœê·¸ í´ë¼ìš°ë“œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from wordcloud import WordCloud 


# ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
import seaborn as sns

from matplotlib import font_manager, rc 


def save_and_get_url(plot_func, filename, static_folder):
    """ Matplotlib ê·¸ë˜í”„ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  URLì„ ë°˜í™˜í•©ë‹ˆë‹¤. """
    if not static_folder:
        return None

    try:
        # --- âš ï¸ [ìˆ˜ì •] ---
        # 1. 'static' í´ë” ì•ˆì— 'img' í´ë” ê²½ë¡œë¥¼ ë§Œë“­ë‹ˆë‹¤.
        img_save_path = os.path.join(static_folder, 'img')

        # 2. 'static/img' í´ë”ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        if not os.path.exists(img_save_path):
            os.makedirs(img_save_path)
        # ---------------------

        plot_object = plot_func()

        if plot_object is None:
             return None

        # 3. íŒŒì¼ ì €ì¥ ê²½ë¡œë¥¼ 'static/img/íŒŒì¼ì´ë¦„'ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
        filepath = os.path.join(img_save_path, filename) # ğŸ‘ˆ static/img/íŒŒì¼ì´ë¦„

        if os.path.exists(filepath):
            os.remove(filepath)

        plot_object.savefig(filepath, dpi=100)
        plt.close('all')        # 4. ë¸Œë¼ìš°ì €ê°€ ì´ë¯¸ì§€ë¥¼ ìš”ì²­í•  URLë„ '/static/img/íŒŒì¼ì´ë¦„'ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
        return f"/static/img/{filename}" # ğŸ‘ˆ /static/img/íŒŒì¼ì´ë¦„

    except Exception as e:
        print(f"-> âŒ ì‹œê°í™” ì €ì¥ ë° URL ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({filename}): {e}")
        plt.close('all')
        return None

# ----------------------------------------------------
# --- âš ï¸ì„¤ì • (Configuration) ---
NAVER_CLIENT_ID = "oo" 
NAVER_CLIENT_SECRET = "oo" 
# ----------------------------------------------------

# ğŸš¨ğŸš¨ ë°ì´í„° ìˆ˜ì§‘ ê°¯ìˆ˜ ì„¤ì • ë¶€ë¶„ ğŸš¨ğŸš¨
MAX_RESULTS_PER_API = 1000 


# --- ìˆœìˆ˜ Python ê°ì„± ì‚¬ì „ ì •ì˜ (Lexicon) ---
POSITIVE_WORDS = [
    'ì¢‹ì•„ìš”', 'ìµœê³ ', 'ë§Œì¡±', 'ì¶”ì²œ', 'ê°•ë ¥ì¶”ì²œ', 'ëŒ€ë°•', 'ì˜ˆì˜', 'ì˜ˆì˜ë‹¤', 'í¸ì•ˆ', 'í¸ì•ˆí•¨',
    'í–‰ë³µ', 'ê°ì‚¬', 'ê¸°ì¨', 'í›Œë¥­', 'ì‚¬ë‘', 'ì¬ë¯¸', 'ì¦ê±°ì›€', 'ì„±ê³µ', 'í•©ê²©', 'ì„ ë¬¼',
    'ë”°ëœ»', 'ë°ì€', 'ì™„ë²½', 'ì¸ìƒí…œ', 'ê°€ì„±ë¹„', 'ì°©í•œ', 'ë†€ë', 'ê¸°ëŒ€', 'ë³´ëŒ', 'ê¹¨ë—',
    'ì‹±ê·¸ëŸ½', 'í”„ë¦¬ë¯¸ì—„', 'ì¹œì ˆ', 'ì‹ ë¢°', 'í¸ë¦¬', 'ì•ˆì •ì„±', 'í’ë¶€', 'íš¨ìœ¨ì ', 'ì„¸ë ¨', 'í”„ë Œë“¤ë¦¬',
    'ë¯¿ìŒ', 'ê°€ì¹˜', 'ë§Œì¡±ê°', 'ì¶”ì²œí•©ë‹ˆë‹¤', 'ì§±', 'ìµœê³ ì˜ˆìš”', 'ì¢‹ë„¤ìš”', 'ê°ë™', 'ì¬êµ¬ë§¤', 'ì¬êµ¬ë§¤ì˜ì‚¬',
    'ë§Œì¡±ìŠ¤ëŸ¬ì›€', 'ê³ ê¸‰', 'í€„ë¦¬í‹°', 'ë›°ì–´ë‚¨', 'ë§Œì¡±ìŠ¤ëŸ½ë‹¤', 'í›Œë¥­í•´ìš”', 'ë¯¿ì„ë§Œ', 'í¸ì•ˆí•´ìš”', 'ì‚¬ë‘ìŠ¤ëŸ¬', 'ì‚¬ë‘ìŠ¤ëŸ¬ì›€',
    'í¸ë¦¬í•¨', 'ì•ˆì •ì ', 'í’ì„±', 'ë§Œì¡±ë„', 'íš¨ìœ¨ì„±', 'ì¸ê¸°', 'ìœ ìš©', 'ì‹¤ìš©ì ', 'ê°•ì¶”', 'ê°•ì¶”í•©ë‹ˆë‹¤',
    'ì¶”ì²œí•´ìš”', 'ê°ì‚¬í•´ìš”', 'ê¸°ë»ìš”', 'ë§Œì¡±í–ˆë‹¤', 'ì¢‹ìŠµë‹ˆë‹¤', 'ìµœê³ ì„', 'í¸ì•ˆí•¨ì´', 'ì¹œì ˆí•´ìš”', 'ê°€ì„±ë¹„ì¢‹', 'ê²½ì œì ',
    'ë§Œì¡±ë„ê°€', 'ë§Œì¡±í–ˆë˜', 'ê¹”ë”', 'ê¹”ë”í•¨', 'ì‹ ì†', 'ì‹ ì†í•¨', 'ì •í™•', 'ì •í™•í•¨', 'í¸ì•ˆí•œ', 'í¬ê·¼',
    'ë‹¬ì½¤', 'ì‹œì›', 'í’ë¯¸', 'í™œë ¥', 'ì•ˆì •ê°', 'ë§Œì¡±ìŠ¤ëŸ¬ìš´', 'ë†€ëë‹¤', 'ê¸°ëŒ€ì´ìƒ', 'ì¶”ì²œë°›ìŒ', 'ê¸°ëŒ€ë§Œí¼'
]

NEGATIVE_WORDS = [
    'ë³„ë¡œ', 'ì‹¤ë§', 'ì•„ì‰¬ì›€', 'ë‚˜ì˜', 'ë¶ˆë§Œ', 'ë¶ˆí¸', 'ìµœì•…', 'í˜ë“¤', 'ê±±ì •', 'ë¬¸ì œ',
    'ì–´ë ¤ì›€', 'ë¶€ì¡±', 'ì‹¤íŒ¨', 'ë‚­ë¹„', 'ì“°ë ˆê¸°', 'ë¹„ì‹¸', 'ë–¨ì–´ì§', 'ì‹¤ë§ìŠ¤ëŸ½', 'ì§€ë£¨', 'í›„íšŒ',
    'ì´ìƒ', 'ì˜¤ë¥˜', 'ì§œì¦', 'ê³ í†µ', 'ë…¼ë€', 'ë¶€ì •ì ', 'ì•½ì ', 'ê²°í•¨', 'ë³µì¡', 'ë¶ˆí•„ìš”',
    'ë¬´ì„±ì˜', 'ë¶ˆì¹œì ˆ', 'ì§€ì €ë¶„', 'ê³¼ëŒ€ê´‘ê³ ', 'ì˜¤ë˜ê±¸ë¦¼', 'í—ˆì ‘', 'ë²ˆê±°ë¡œì›€', 'ë¶€ì •í™•', 'ë¹„ì¶”ì²œ', 'ë¶ˆì‹ ',
    'ë¶ˆë§Œì¡±', 'ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ì›€', 'ë¶ˆì¾Œ', 'ë¶ˆë§Œì¡±í•˜ë‹¤', 'ë¶ˆë§Œì¡±ìŠ¤ëŸ½ë‹¤', 'ë¶ˆì•ˆì •', 'ì§ˆë‚®', 'ì €ê¸‰', 'í˜•í¸ì—†', 'ì—‰ë§',
    'ë§í•¨', 'ì‹¤íŒ¨ì‘', 'ë¶ˆí¸í•¨', 'ì„±ê°€ì‹¬', 'ë²ˆê±°ë¡œì›Œìš”', 'ëª»í•¨', 'ë¶€ì‹¤', 'ë¶ˆëŸ‰', 'ì†í•´', 'ë¶ˆì¹œì ˆí•¨',
    'ë¹„ì¶”', 'ë¹„ì¶”í•©ë‹ˆë‹¤', 'ë¬´ì±…ì„', 'ì·¨ì•½', 'ë°˜í’ˆ', 'í™˜ë¶ˆ', 'ë¶ˆë§Œì¡±í–ˆë‹¤', 'ë¶€ì‘ìš©', 'ë¬¸ì œìˆ', 'ë¶ˆí¸í–ˆë‹¤',
    'ì§œì¦ë‚˜ìš”', 'ì‚¬ìš©ë¶ˆê°€', 'ë¨¹ë¨¹', 'ë¶ˆí¸í•œ', 'ëª»ì“°ê² ë‹¤', 'ê³ ì¥', 'í„°ë¬´ë‹ˆì—†', 'ë¶ˆë§Œì¡±ê°', 'ë¶ˆí¸í•¨ì´', 'ë¶ˆì¾Œê°',
    'ë¶ˆí•©ë¦¬', 'ì•…í™”', 'ì•…ì„±', 'ë¶€ì •', 'ë¶€ì ì ˆ', 'ì‹¤ë§ìŠ¤ëŸ¬ì› ë‹¤', 'ì‹¤ë§í–ˆë‹¤', 'í›„íšŒí•œë‹¤', 'ì‹¤ë§í•´ìš”', 'ìµœì•…ì´ë‹¤',
    'ìµœì•…ì´ì—ìš”', 'í˜•í¸ì—†ë‹¤', 'ì—‰ë§ì§„ì°½', 'ì—‰ì„±', 'ê°’ë¹„ì‹¼', 'ë¶ˆì¹œì ˆí–ˆì–´ìš”', 'ë¶ˆí¸í–ˆìŠµë‹ˆë‹¤', 'ë¶€ì •í™•í•¨', 'ë¶€ì£¼ì˜', 'ë¬¸ì œê°€ìˆë‹¤'
]
# --- í°íŠ¸ ì„¤ì • (í†µí•© ë¡œì§) ---
# 1. ì‹œìŠ¤í…œì— ì„¤ì¹˜ëœ í•œê¸€ í°íŠ¸ ì°¾ê¸°
def get_korean_font():
    """ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸(Malgun Gothic, AppleGothic, NanumGothic ë“±)ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    font_names = ['Malgun Gothic', 'AppleGothic', 'NanumGothic', 'Noto Sans CJK JP']
    for font_name in font_names:
        font_path = font_manager.findfont(font_name)
        if font_path:
            return font_name, font_path
    return None, None

korean_font_name, korean_font_path = get_korean_font()

if korean_font_name:
    # 2. í°íŠ¸ ìºì‹œ ì´ˆê¸°í™” (í•„ìš”ì‹œ)
    try:
        # ì´ ë¶€ë¶„ì€ í™˜ê²½ì— ë”°ë¼ ìºì‹œê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        pass
    except Exception:
        pass
    
    # 3. Matplotlibì— í°íŠ¸ ì„¤ì • ì ìš©
    # âš ï¸ [ìœ ì§€] font_manager.fontManager.addfont í˜¸ì¶œì€ í™˜ê²½ì— ë”°ë¼ ì—ëŸ¬ë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆì§€ë§Œ, 
    # ì•ˆì •ì ì¸ ì‚¬ìš©ì í™˜ê²½ì„ ê°€ì •í•˜ê³  ìœ ì§€í•©ë‹ˆë‹¤.
    # font_manager.fontManager.addfont(korean_font_path)
    rc('font', family=korean_font_name)
    print(f"-> [OK] í•œê¸€ í°íŠ¸ '{korean_font_name}' ì„¤ì • ì™„ë£Œ.")
    FONT_PATH = korean_font_path # ì›Œë“œí´ë¼ìš°ë“œìš© ê²½ë¡œ ì„¤ì •
else:
    print("[WARNING] í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    FONT_PATH = None
    
plt.rcParams['axes.unicode_minus'] = False # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
# ----------------------------------------------------

# ----------------------------------------------------
# --- ê²€ìƒ‰ì–´ ìœ íš¨ì„± ë° ë¸Œëœë“œëª… ê²€ì¦ í•¨ìˆ˜ ---
# ----------------------------------------------------

def is_valid_query(query):
    """ ë¬´ì˜ë¯¸í•œ ë°˜ë³µ ë¬¸ìì—´, ë„ˆë¬´ ì§§ì€ ë¬¸ìì—´ ë“±ì„ ê±¸ëŸ¬ë‚´ ë¶„ì„ì„ ìœ„í•œ ê²€ìƒ‰ì–´ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤. """
    if len(query.replace(' ', '')) < 2:
        print("-> âŒ ê²€ìƒ‰ì–´ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. (ê³µë°± ì œì™¸ 2ì ë¯¸ë§Œ)")
        return False
    # âš ï¸ [ìˆ˜ì •] ì •ê·œì‹ íŒ¨í„´ì´ ì¼ë¶€ í™˜ê²½ì—ì„œ ì˜¤ë¥˜ë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆì–´, Raw Stringìœ¼ë¡œ ëª…ì‹œí•©ë‹ˆë‹¤.
    repeated_pattern = re.compile(r'([ê°€-í£a-zA-Z0-9])\1{3,}')
    if repeated_pattern.search(query):
        print("-> âŒ ë™ì¼í•œ ë¬¸ìê°€ 4íšŒ ì´ìƒ ë°˜ë³µë˜ëŠ” íŒ¨í„´ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. (ë¬´ì˜ë¯¸í•œ ê²€ìƒ‰ì–´ë¡œ íŒë‹¨)")
        return False
    return True

def is_brand_name(query, client_id, client_secret, confidence_threshold=0.6):
    """ ë„¤ì´ë²„ ì‡¼í•‘ APIë¥¼ ì´ìš©í•´ ê²€ìƒ‰ì–´ê°€ ë¸Œëœë“œëª…ì¸ì§€ êµì°¨ ê²€ì¦í•©ë‹ˆë‹¤. """
    print(f"\n--- 0ë‹¨ê³„: '{query}'ê°€ ë¸Œëœë“œëª…ì¸ì§€ ê²€ì¦ ì‹œì‘ ---")
    url = "https://openapi.naver.com/v1/search/shop.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {"query": query, "display": 20} 

    try:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        data = response.json()
        items = data.get("items", [])

        if not items:
            print("-> ì‡¼í•‘ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ë¸Œëœë“œëª…ìœ¼ë¡œ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        brands = [item.get("brand") for item in items if item.get("brand")]
        if not brands:
            print("-> ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë¸Œëœë“œëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        brands = [b.strip() for b in brands] # âš ï¸ [ì¶”ê°€] ë¸Œëœë“œëª… ì•ë’¤ ê³µë°± ì œê±°
        
        brand_counts = Counter(brands)
        most_common_brand, count = brand_counts.most_common(1)[0]
        dominance_ratio = count / len(brands)
        
        is_dominant = dominance_ratio >= confidence_threshold
        # âš ï¸ [ìˆ˜ì •] ê²€ìƒ‰ì–´ì™€ ë¸Œëœë“œëª…ì„ ë¹„êµí•  ë•Œ, ì†Œë¬¸ìí™”í•˜ì—¬ ì •í™•í•œ ë¹„êµë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        is_query_matched = most_common_brand.lower() == query.lower() 

        print(f"-> ê°€ì¥ ë§ì´ ë…¸ì¶œëœ ë¸Œëœë“œ: '{most_common_brand}' (ë¹„ì¤‘: {dominance_ratio:.2%})")

        if is_dominant and is_query_matched:
            print(f"-> âœ… ë¸Œëœë“œëª… ì¼ì¹˜ ë° ë¹„ì¤‘({dominance_ratio:.2%}) í†µê³¼.")
            return True
        else:
            reason = []
            if not is_dominant:
                reason.append(f"ë¸Œëœë“œ ë¹„ì¤‘({dominance_ratio:.2%})ì´ ê¸°ì¤€ì¹˜({confidence_threshold:.0%}) ë¯¸ë§Œ")
            if not is_query_matched:
                reason.append(f"ê°€ì¥ ì§€ë°°ì ì¸ ë¸Œëœë“œ('{most_common_brand}')ê°€ ê²€ìƒ‰ì–´ì™€ ë¶ˆì¼ì¹˜")
            
            print(f"-> âŒ ë¸Œëœë“œëª…ìœ¼ë¡œ íŒë‹¨í•˜ê¸° ì–´ë ¤ì›€ ({', '.join(reason)}).")
            return False

    except requests.exceptions.RequestException as e:
        # âš ï¸ [ìˆ˜ì •] ì˜¤ë¥˜ ë°œìƒ ì‹œ 500 ì—ëŸ¬ë¥¼ ë°˜í™˜í•´ì•¼ í•˜ë¯€ë¡œ, í•¨ìˆ˜ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.
        print(f"ì‡¼í•‘ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

# ----------------------------------------------------
# --- ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜ ---
# ----------------------------------------------------

def fetch_naver_search_results(query, api_type, client_id, client_secret, total_results):
    """ Blog ë˜ëŠ” News ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. """
    print(f"\n--- 1ë‹¨ê³„: '{api_type.capitalize()}' ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {total_results}ê±´) ---")

    if api_type == 'blog':
        url_template = "https://openapi.naver.com/v1/search/blog.json"
    elif api_type == 'news':
        url_template = "https://openapi.naver.com/v1/search/news.json"
    else:
        return pd.DataFrame() 

    # âš ï¸ [ìˆ˜ì •] ìµœì¢… DFëŠ” ì¼ê´€ëœ ì»¬ëŸ¼ëª…ì„ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.
    df_columns = ['title', 'link', 'description', 'channel_name', 'postdate', 'channel_type'] 
    df = pd.DataFrame(columns=df_columns)
    display = 100
    remove_tag = re.compile('<.*?>')

    for start in range(1, min(total_results, 1001) + 1, display): 
        # âš ï¸ [ìˆ˜ì •] queryë¥¼ URL ì¸ì½”ë”©í•˜ì§€ ì•Šì•„ë„ requests ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•´ì¤ë‹ˆë‹¤.
        url = url_template
        headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret
        }
        params = {
            "query": query,
            "display": display,
            "start": start
        }

        try:
            response = requests.get(url, headers=headers, params=params)
            response.raise_for_status()
            data = response.json()
            items = data.get("items", [])
            
            if not items:
                break

            processed_items = []
            for item in items:
                clean_item = {}
                clean_item['title'] = re.sub(remove_tag, '', item.get('title', ''))
                clean_item['description'] = re.sub(remove_tag, '', item.get('description', ''))
                clean_item['link'] = item.get('link', '') 
                
                # âš ï¸ [ìˆ˜ì •] ì±„ë„ëª… ë° ë‚ ì§œ ì²˜ë¦¬ ë¡œì§ í†µì¼
                if api_type == 'blog':
                    clean_item['channel_name'] = re.sub(remove_tag, '', item.get('bloggername', ''))
                    raw_date = item.get('postdate', '')
                    clean_item['postdate'] = pd.to_datetime(raw_date, format='%Y%m%d', errors='coerce').normalize() # YYYYMMDD í˜•ì‹
                elif api_type == 'news':
                    clean_item['channel_name'] = item.get('publisher', '') 
                    raw_date = item.get('pubDate', '')
                    # RFC 822 í˜•ì‹ ì²˜ë¦¬ (ì˜ˆ: Wed, 25 Oct 2023 00:00:00 +0900)
                    clean_item['postdate'] = pd.to_datetime(raw_date, format='%a, %d %b %Y %H:%M:%S +0900', errors='coerce').normalize()
                    
                clean_item['channel_type'] = api_type # ì±„ë„ íƒ€ì… ì¶”ê°€

                processed_items.append(clean_item)

            temp_df = pd.DataFrame(processed_items)
            df = pd.concat([df, temp_df], ignore_index=True)

        except requests.exceptions.RequestException as e:
            print(f"{api_type.capitalize()} API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (start={start}): {e}")
            break 

    df = df.dropna(subset=['postdate']).drop_duplicates()
    df = df[df['postdate'] <= datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)]
    print(f"-> ì´ {len(df)}ê±´ì˜ '{api_type.capitalize()}' ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    return df


def get_search_trend(query, client_id, client_secret):
    """ ë„¤ì´ë²„ ë°ì´í„°ë© APIë¥¼ ì´ìš©í•´ ìµœê·¼ 1ë…„ê°„ì˜ ì›”ê°„ ê²€ìƒ‰ëŸ‰ ì¶”ì´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. """
    # âš ï¸ [ìˆ˜ì •] ì£¼ê°„(90ì¼) ëŒ€ì‹  ì›”ê°„(365ì¼) íŠ¸ë Œë“œë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
    print(f"\n--- 2ë‹¨ê³„: '{query}' ì›”ê°„ ê²€ìƒ‰ëŸ‰ ì¶”ì´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ìµœê·¼ 1ë…„) ---")

    url = "https://openapi.naver.com/v1/datalab/search"
    end_date = datetime.now().strftime('%Y-%m-%d')
    start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d') # ìµœê·¼ 1ë…„

    body = json.dumps({
        "startDate": start_date,
        "endDate": end_date,
        "timeUnit": "month",  # âš ï¸ [ìˆ˜ì •] ì›”ê°„ ë‹¨ìœ„ ìš”ì²­
        "keywordGroups": [{"groupName": query, "keywords": [query]}]
    })
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
        "Content-Type": "application/json"
    }

    try:
        response = requests.post(url, headers=headers, data=body)
        response.raise_for_status()
        data = response.json()

        if not data.get('results') or not data['results'][0].get('data'):
             print("-> âŒ ë°ì´í„°ë© API ì‘ë‹µì— ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
             return pd.DataFrame(columns=['date', 'ratio']) # âš ï¸ [ìˆ˜ì •] ë°˜í™˜ ì»¬ëŸ¼ëª… í†µì¼

        trend_df = pd.DataFrame(data['results'][0]['data'])
        # âš ï¸ [ìˆ˜ì •] ì»¬ëŸ¼ëª…ì„ 'date', 'ratio'ë¡œ í†µì¼í•˜ì—¬ êµì°¨ ë¶„ì„ í•¨ìˆ˜ì—ì„œ ì‚¬ìš©í•˜ê¸° ì‰½ê²Œ í•©ë‹ˆë‹¤.
        trend_df = trend_df.rename(columns={'period': 'date', 'ratio': 'ratio'}) 
        
        trend_df['date'] = pd.to_datetime(trend_df['date']) 
        
        print(f"-> ìµœê·¼ 1ë…„ê°„ì˜ ì›”ê°„ ê²€ìƒ‰ëŸ‰ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤. (ì´ {len(trend_df)}ê±´)")
        return trend_df[['date', 'ratio']]

    except requests.exceptions.RequestException as e:
        print(f"ë°ì´í„°ë© API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame(columns=['date', 'ratio'])
    except Exception as e:
        print(f"ë°ì´í„°ë© ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame(columns=['date', 'ratio'])
    
# ----------------------------------------------------
# --- ì‹œê°í™” í•¨ìˆ˜ (í”Œë¡¯ ê°ì²´ ìƒì„±) ---
# ----------------------------------------------------

# âš ï¸ [ìˆ˜ì •] ì´ í•¨ìˆ˜ ì „ì²´ë¥¼ ë³µì‚¬í•´ì„œ ê¸°ì¡´ í•¨ìˆ˜ë¥¼ ë®ì–´ì“°ì„¸ìš”.

def visualize_post_frequency(df, frequency_type='monthly'): 
    """ ë‹¨ì¼ í†µí•© ë°ì´í„°í”„ë ˆì„ì„ ë°›ì•„ ì›”ë³„ ë˜ëŠ” ì£¼ë³„ ì–¸ê¸‰ëŸ‰ ì¶”ì´ë¥¼ ì‹œê°í™”í•˜ê³  í”Œë¡¯ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. """
    time_unit = "ì›”ë³„" if frequency_type == 'monthly' else "ì£¼ë³„"
    time_span = "12ê°œì›”" if frequency_type == 'monthly' else "6ê°œì›”"
    color = 'darkorange' if frequency_type == 'monthly' else 'purple'
    
    # âš ï¸ [ìˆ˜ì •] ì œëª©ì—ì„œ "(í˜„ì¬ ê¸°ê°„ ì œì™¸)" ë¬¸êµ¬ ì‚­ì œ
    print(f"\n--- 3ë‹¨ê³„ ë¶„ì„: {time_unit} ì–¸ê¸‰ëŸ‰ ì‹œê°í™” ({time_span}) ---")
    
    temp_df = df.copy()
    
    try:
        temp_df['postdate'] = pd.to_datetime(temp_df['postdate'], errors='coerce').dt.normalize()
    except Exception:
        print(f"ê²½ê³ : ë‚ ì§œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ë¶„ì„ì„ ê±´ë„ˆëœ•ë‹ˆë‹¤.")
        return None  # None ë°˜í™˜ìœ¼ë¡œ í†µì¼
    
    temp_df.dropna(subset=['postdate'], inplace=True)
    if temp_df.empty:
        print("ê²½ê³ : ìœ íš¨í•œ ë‚ ì§œë¥¼ ê°€ì§„ ê²Œì‹œë¬¼ ë°ì´í„°ê°€ ì—†ì–´ ë¶„ì„ì„ ê±´ë„ˆëœ•ë‹ˆë‹¤.")
        return None
        
    now = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    
    if frequency_type == 'monthly':
        
        # âš ï¸ [ìˆ˜ì •] í˜„ì¬ ì›”ì„ ì œì™¸í•˜ëŠ” í•„í„°ë¥¼ ì œê±°í•˜ê³ , ëª¨ë“  ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        # df_filtered = temp_df[temp_df['postdate'] < current_month_start].copy() # <-- ê¸°ì¡´ ì½”ë“œ
        df_filtered = temp_df.copy() # <-- ìˆ˜ì •ëœ ì½”ë“œ (ëª¨ë“  ë°ì´í„° ì‚¬ìš©)
        
        start_date_offset = pd.DateOffset(months=12)
        freq_label = 'post_month'
        freq_unit = 'MS'
        
        if not df_filtered.empty:
            # ìµœê·¼ 12ê°œì›” ë°ì´í„°ë§Œ í•„í„°ë§ (ìµœì‹  ë‚ ì§œ ê¸°ì¤€)
            latest_date = df_filtered['postdate'].max()
            # [ì¶”ê°€] í˜¹ì‹œ ëª¨ë¥¼ ë¯¸ë˜ ë‚ ì§œ ë°ì´í„° ë°©ì§€ë¥¼ ìœ„í•´ nowë¡œ ìƒí•œì„  ì ìš©
            latest_date_cap = min(latest_date, now)
            
            # 12ê°œì›” ì „ ë°ì´í„°ë§Œ í•„í„°ë§
            df_filtered = df_filtered[
                (df_filtered['postdate'] >= (latest_date_cap - start_date_offset)) &
                (df_filtered['postdate'] <= latest_date_cap) # âš ï¸ [ì¶”ê°€] í˜„ì¬ ë‚ ì§œ ìƒí•œì„ 
            ].copy()
            
            df_filtered[freq_label] = df_filtered['postdate'].dt.strftime('%Y-%m')
            counts_raw = df_filtered[freq_label].value_counts().sort_index()
            
            if counts_raw.empty: # âš ï¸ [ì¶”ê°€] í•„í„°ë§ í›„ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ
                 print(f"ê²½ê³ : í•„í„°ë§ ê²°ê³¼ ìµœê·¼ {time_span} ì´ë‚´ì˜ ìœ íš¨í•œ ê²Œì‹œë¬¼ì´ ì—†ì–´ {time_unit} ë¶„ì„ì„ ê±´ë„ˆëœ•ë‹ˆë‹¤.")
                 return None

            # ë¹ˆ ì›”ì„ ì±„ìš°ê¸° ìœ„í•œ ì „ì²´ ë ˆì´ë¸” ìƒì„±
            start_month = counts_raw.index.min()
            end_month = counts_raw.index.max()
            
            full_index_range = pd.date_range(start=start_month, end=end_month, freq=freq_unit)
            full_labels = full_index_range.strftime('%Y-%m')
            rotation_angle = 45
        else:
            print(f"ê²½ê³ : í•„í„°ë§ ê²°ê³¼ ìµœê·¼ {time_span} ì´ë‚´ì˜ ìœ íš¨í•œ ê²Œì‹œë¬¼ì´ ì—†ì–´ {time_unit} ë¶„ì„ì„ ê±´ë„ˆëœ•ë‹ˆë‹¤.")
            return None # None ë°˜í™˜ìœ¼ë¡œ í†µì¼

    elif frequency_type == 'weekly':
        # (ì£¼ë³„ ë¶„ì„ ë¡œì§ì€ ë¡œê·¸ìƒ ì •ìƒ ì‘ë™í–ˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤)
        current_week_start = (now - timedelta(days=now.weekday()))
        
        df_filtered = temp_df[temp_df['postdate'] < current_week_start].copy()
        start_date_offset = pd.DateOffset(months=6) 

        if not df_filtered.empty:
            latest_date = df_filtered['postdate'].max()
            df_filtered = df_filtered[df_filtered['postdate'] >= (latest_date - start_date_offset)].copy()
            
            if df_filtered.empty: # âš ï¸ [ì¶”ê°€] í•„í„°ë§ í›„ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ
                print(f"ê²½ê³ : í•„í„°ë§ ê²°ê³¼ ìµœê·¼ {time_span} ì´ë‚´ì˜ ìœ íš¨í•œ ê²Œì‹œë¬¼ì´ ì—†ì–´ {time_unit} ë¶„ì„ì„ ê±´ë„ˆëœ•ë‹ˆë‹¤.")
                return None

            df_resample = df_filtered.set_index('postdate')
            counts_resampled = df_resample.resample('W').size() 
            
            full_counts = counts_resampled
            
            full_counts.index = full_counts.index.strftime('%Y-%m-%d')
            full_labels = full_counts.index.tolist() 
            rotation_angle = 90
        else:
            print(f"ê²½ê³ : í•„í„°ë§ ê²°ê³¼ ìµœê·¼ {time_span} ì´ë‚´ì˜ ìœ íš¨í•œ ê²Œì‹œë¬¼ì´ ì—†ì–´ {time_unit} ë¶„ì„ì„ ê±´ë„ˆëœ•ë‹ˆë‹¤.")
            return None 
    
    else:
        return None

    # âš ï¸ [ìˆ˜ì •] 'full_counts'ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ locals() ì²´í¬ ì œê±°
    # 'monthly'ì˜ ê²½ìš° full_countsê°€ ì—¬ê¸°ì„œ ì •ì˜ë˜ë¯€ë¡œ, counts_rawë¡œ ëŒ€ì‹  ì²´í¬
    if frequency_type == 'monthly' and (not 'counts_raw' in locals() or counts_raw.empty): 
        print(f"ê²½ê³ : í•„í„°ë§ ê²°ê³¼ ìµœê·¼ {time_span} ì´ë‚´ì˜ ìœ íš¨í•œ ê²Œì‹œë¬¼ì´ ì—†ì–´ {time_unit} ë¶„ì„ì„ ê±´ë„ˆëœ•ë‹ˆë‹¤.")
        return None
    elif frequency_type == 'weekly' and (not 'full_counts' in locals() or full_counts.empty):
        print(f"ê²½ê³ : í•„í„°ë§ ê²°ê³¼ ìµœê·¼ {time_span} ì´ë‚´ì˜ ìœ íš¨í•œ ê²Œì‹œë¬¼ì´ ì—†ì–´ {time_unit} ë¶„ì„ì„ ê±´ë„ˆëœ•ë‹ˆë‹¤.")
        return None

    if frequency_type == 'monthly':
        counts_series = counts_raw.rename('count')
        full_counts = counts_series.reindex(full_labels, fill_value=0)
    
    # ì‹œê°í™” ì‹¤í–‰
    # âš ï¸ [ìˆ˜ì •] fig ë³€ìˆ˜ì— í”Œë¡¯ ê°ì²´ë¥¼ í• ë‹¹
    fig = plt.figure(figsize=(15 if frequency_type == 'weekly' else 12, 6))
    sns.lineplot(
        x=full_counts.index, y=full_counts.values, marker='o', color=color
    )

    # âš ï¸ [ìˆ˜ì •] ì œëª©ì—ì„œ "(í˜„ì¬ ... ì œì™¸)" ë¬¸êµ¬ ì‚­ì œ
    plt.title(f'ìµœê·¼ {time_span} ì–¸ê¸‰ëŸ‰ ì¶”ì´', fontsize=16) 
    
    if frequency_type == 'weekly':
        plt.xlabel(f'ì–¸ê¸‰ {time_unit} (ì£¼ì°¨ ì¢…ë£Œì¼)', fontsize=12) 
    else:
        plt.xlabel(f'ì–¸ê¸‰ {time_unit}', fontsize=12) 
        
    plt.ylabel('ì´ ì–¸ê¸‰ëŸ‰', fontsize=12)

    plt.xticks(full_counts.index, rotation=rotation_angle) 
    plt.grid(axis='y', linestyle='--')
    plt.tight_layout()
    
    print(f"-> í†µí•© ë°ì´í„° {time_unit} ì–¸ê¸‰ëŸ‰ ì‹œê°í™” í”Œë¡¯ ê°ì²´ ìƒì„± ì™„ë£Œ.")
    
    # âš ï¸ [ìˆ˜ì •] fig ê°ì²´ë¥¼ ë°˜í™˜ (save_and_get_url í˜¸í™˜)
    return fig

def visualize_combined_trend(total_df, trend_df):
    """ ê²€ìƒ‰ëŸ‰(ì›”ê°„)ê³¼ ì–¸ê¸‰ëŸ‰(ì›”ê°„)ì„ í•˜ë‚˜ì˜ ê·¸ë˜í”„ì— ì‹œê°í™”í•˜ê³  í”Œë¡¯ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. """
    print("\n--- 4ë‹¨ê³„ ë¶„ì„: ì–¸ê¸‰ëŸ‰ vs ê²€ìƒ‰ëŸ‰ í†µí•© ì‹œê°í™” (ìµœê·¼ 1ë…„ ì›”ê°„ ë‹¨ìœ„) ---")
    
    if trend_df.empty or 'date' not in trend_df.columns:
        print("ê²½ê³ : ê²€ìƒ‰ëŸ‰(ë°ì´í„°ë©) ë°ì´í„°ê°€ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜ëª»ë˜ì–´ í†µí•© ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
        
    temp_df = total_df.copy()
    
    try:
        temp_df['postdate'] = pd.to_datetime(temp_df['postdate'], errors='coerce').dt.normalize()
        temp_df.dropna(subset=['postdate'], inplace=True)
    except Exception:
        return

    temp_df['month_start_date'] = temp_df['postdate'].dt.to_period('M').apply(lambda x: x.start_time).dt.normalize() 
    mention_counts = temp_df['month_start_date'].value_counts().reset_index()
    mention_counts.columns = ['date', 'mention_count']
    
    trend_df['date'] = trend_df['date'].dt.to_period('M').apply(lambda x: x.start_time).dt.normalize()
    
    start_date = trend_df['date'].min()
    end_date = trend_df['date'].max()
    mention_counts = mention_counts[
        (mention_counts['date'] >= start_date) & (mention_counts['date'] <= end_date)
    ]
    
    combined_df = pd.merge(trend_df, mention_counts, on='date', how='left').fillna(0)
    
    max_mention = combined_df['mention_count'].max()
    combined_df['mention_ratio'] = (combined_df['mention_count'] / max_mention) * 100 if max_mention > 0 else 0.0
    
    # ì‹œê°í™” (Dual Axis)
    fig, ax1 = plt.subplots(figsize=(12, 6))
    x_labels = combined_df['date'].dt.strftime('%Y-%m')

    color = 'tab:red'
    ax1.set_xlabel('ì›”ê°„ (ì‹œì‘ì¼ ê¸°ì¤€)', fontsize=12)
    ax1.set_ylabel('ê²€ìƒ‰ëŸ‰ ë¹„ìœ¨ (0-100) - Datalab ê¸°ì¤€', color=color, fontsize=12)
    sns.lineplot(x=combined_df.index, y='ratio', data=combined_df, marker='o', color=color, ax=ax1, label='ê²€ìƒ‰ëŸ‰')
    ax1.tick_params(axis='y', labelcolor=color)
    ax1.set_ylim(0, 100)
    ax1.set_xticks(combined_df.index)
    ax1.set_xticklabels(x_labels, rotation=45, ha='right')
    ax1.legend(loc='upper left')

    ax2 = ax1.twinx() 
    color = 'tab:blue'
    ax2.set_ylabel('ì–¸ê¸‰ëŸ‰ ë¹„ìœ¨ (0-100) - ìì²´ ìµœëŒ€ ì–¸ê¸‰ëŸ‰ ê¸°ì¤€', color=color, fontsize=12) 
    sns.lineplot(x=combined_df.index, y='mention_ratio', data=combined_df, marker='s', color=color, ax=ax2, label='ì–¸ê¸‰ëŸ‰')
    ax2.tick_params(axis='y', labelcolor=color)
    ax2.set_ylim(0, 100)
    ax2.legend(loc='upper right')
    
    plt.title('ìµœê·¼ 1ë…„ ì›”ê°„ ê²€ìƒ‰ëŸ‰ ë° ì–¸ê¸‰ëŸ‰ ì¶”ì´ ë¹„êµ (0-100 ë¹„ìœ¨)', fontsize=16)
    fig.tight_layout()
    print("-> ê²€ìƒ‰ëŸ‰ ë° ì–¸ê¸‰ëŸ‰ í†µí•© ì‹œê°í™” í”Œë¡¯ ê°ì²´ ìƒì„± ì™„ë£Œ.")
    return plt

def visualize_sentiment_word_clouds(df, positive_words, negative_words):
    """ ê°ì„± ì‚¬ì „ ê¸°ë°˜ ê¸ì •/ë¶€ì • ì›Œë“œ í´ë¼ìš°ë“œ í”Œë¡¯ ê°ì²´ì™€ ìƒìœ„ 7ê°œ í‚¤ì›Œë“œ DFë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. """
    print("\n--- 5ë‹¨ê³„ ë¶„ì„: ê°ì„± ì‚¬ì „ ê¸°ë°˜ ê¸ì •/ë¶€ì • ë° í†µí•© í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘ ---")
    
    if df.empty or 'description' not in df.columns:
        return (None, None, None, pd.DataFrame())

    text = ' '.join(df['description'].astype(str).tolist())
    cleaned_text = re.sub('[^ê°€-í£\s]', '', text).lower()
    
    def get_word_counts(word_list, text_corpus, max_words=20):
        counts = {}
        for word in word_list:
            count = len(re.findall(r'\b' + re.escape(word) + r'\b', text_corpus)) 
            if count > 0:
                counts[word] = count
        return dict(sorted(counts.items(), key=lambda item: item[1], reverse=True)[:max_words])
    
    def create_wordcloud_plot(counts, title, colormap):
        if not counts:
            return None

        wc = WordCloud(
            font_path=FONT_PATH, width=800, height=400, background_color='white',
            max_words=20, colormap=colormap, prefer_horizontal=0.9, collocations=False 
        )
        wordcloud = wc.generate_from_frequencies(counts)
        
        fig = plt.figure(figsize=(12, 7)) 
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off') 
        plt.title(title, fontsize=16)
        plt.tight_layout(pad=3.0) 
        
        return fig

    pos_counts = get_word_counts(positive_words, cleaned_text, max_words=20)
    pos_plot = create_wordcloud_plot(pos_counts, 'ê¸ì • ê°ì„± í‚¤ì›Œë“œ íƒœê·¸ í´ë¼ìš°ë“œ (Top 20)', 'YlGn')
    
    neg_counts = get_word_counts(negative_words, cleaned_text, max_words=20)
    neg_plot = create_wordcloud_plot(neg_counts, 'ë¶€ì • ê°ì„± í‚¤ì›Œë“œ íƒœê·¸ í´ë¼ìš°ë“œ (Top 20)', 'Reds_r')

    ALL_SENTIMENT_WORDS = positive_words + negative_words
    all_counts_top20 = get_word_counts(ALL_SENTIMENT_WORDS, cleaned_text, max_words=20)
    all_plot = create_wordcloud_plot(all_counts_top20, 'ê¸ì •+ë¶€ì • í†µí•© í‚¤ì›Œë“œ íƒœê·¸ í´ë¼ìš°ë“œ (Top 20)', 'plasma')
    
    # ìƒìœ„ 20ê°œ í‚¤ì›Œë“œë¥¼ ë°˜í™˜í•˜ë„ë¡ ë³€ê²½
    all_counts_top20_selected = {k: all_counts_top20[k] for k in list(all_counts_top20.keys())[:min(20, len(all_counts_top20))]}
    all_df = pd.DataFrame(all_counts_top20_selected.items(), columns=['í‚¤ì›Œë“œ', 'ì–¸ê¸‰ íšŸìˆ˜'])
    
    print("-> ê°ì„± ì‚¬ì „ ê¸°ë°˜ ì›Œë“œí´ë¼ìš°ë“œ í”Œë¡¯ ê°ì²´ ìƒì„± ë° ìƒìœ„ í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ.")
    return pos_plot, neg_plot, all_plot, all_df

def visualize_competitor_mention_comparison(own_query, own_df, competitor_query, competitor_df):
    """ ìì‚¬/ê²½ìŸì‚¬ ì›”ë³„ ì–¸ê¸‰ëŸ‰ì„ ë¹„êµí•˜ì—¬ ì‹œê°í™”í•˜ê³  í”Œë¡¯ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. """
    print(f"\n--- 9ë‹¨ê³„ ë¶„ì„: ìì‚¬({own_query}) vs ê²½ìŸì‚¬({competitor_query}) ì›”ë³„ ì–¸ê¸‰ëŸ‰ ë¹„êµ ì‹œê°í™” ì‹œì‘ ---")

    if own_df.empty and competitor_df.empty:
        print("ê²½ê³ : ìì‚¬ì™€ ê²½ìŸì‚¬ ëª¨ë‘ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ ë¹„êµ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    def prepare_monthly_counts(df, label):
        temp_df = df.copy()
        if temp_df.empty: return pd.Series([], dtype='int64', name=label)
        try:
            temp_df['postdate'] = pd.to_datetime(temp_df['postdate'], errors='coerce').dt.normalize()
            temp_df.dropna(subset=['postdate'], inplace=True)
            
            now = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
            current_month_start = now.replace(day=1)
            start_date_offset = pd.DateOffset(months=12)
            
            df_filtered = temp_df[temp_df['postdate'] < current_month_start].copy()
            if df_filtered.empty: return pd.Series([], dtype='int64', name=label)
                 
            latest_date = df_filtered['postdate'].max()
            df_filtered = df_filtered[df_filtered['postdate'] >= (latest_date - start_date_offset)].copy()
            
            df_filtered['post_month'] = df_filtered['postdate'].dt.strftime('%Y-%m')
            counts = df_filtered['post_month'].value_counts().sort_index().rename(label)
            return counts
        except Exception as e:
            print(f"ê²½ê³ : {label} ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. {e}")
            return pd.Series([], dtype='int64', name=label)

    own_counts = prepare_monthly_counts(own_df, own_query)
    comp_counts = prepare_monthly_counts(competitor_df, competitor_query)
    combined_counts = pd.concat([own_counts, comp_counts], axis=1).fillna(0)
    combined_counts.index.name = 'Month'
    
    if combined_counts.shape[0] < 2:
        print("ê²½ê³ : ë¹„êµí•  ì›” ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. (ìµœì†Œ 2ê°œì›” í•„ìš”)")
        return None

    plt.figure(figsize=(10, 5))
    sns.lineplot(x=combined_counts.index, y=own_query, data=combined_counts, marker='o', color='tab:blue', label=own_query)
    sns.lineplot(x=combined_counts.index, y=competitor_query, data=combined_counts, marker='s', color='tab:red', label=competitor_query)

    plt.title(f'{own_query} vs {competitor_query} ì›”ë³„ ì–¸ê¸‰ëŸ‰ ì¶”ì´ ë¹„êµ (ìµœê·¼ 12ê°œì›”)', fontsize=14) 
    plt.xlabel('ì›”', fontsize=11) 
    plt.ylabel('ì´ ì–¸ê¸‰ëŸ‰', fontsize=11)
    plt.xticks(combined_counts.index, rotation=45) 
    plt.legend(fontsize=11)
    plt.grid(axis='y', linestyle='--')
    plt.subplots_adjust(left=0.12, right=0.92, top=0.92, bottom=0.15)
    
    print("-> ìì‚¬/ê²½ìŸì‚¬ ì–¸ê¸‰ëŸ‰ ë¹„êµ ì‹œê°í™” í”Œë¡¯ ê°ì²´ ìƒì„± ì™„ë£Œ.")
    return plt

def generate_smart_report(query, total_mentions, sentiment_label, positive_score, top_keywords, outbreak_weeks, trend_available, most_frequent_date, mention_change_rate, api_key):
    """
    Gemini API í˜¸ì¶œ ì¤‘ 5ì´ˆë§ˆë‹¤ ì§„í–‰ ìƒí™©ì„ ì•Œë ¤ì¤ë‹ˆë‹¤.
    ëª¨ë“  ë¶„ì„ ì§€í‘œ(6ê°€ì§€ í•µì‹¬ ìš”ì†Œ)ë¥¼ ì¢…í•©í•˜ì—¬ ì‹¬ì¸µ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    # 1. ë°ì´í„° ì „ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸í™”
    keywords_str = ", ".join([k['í‚¤ì›Œë“œ'] for k in top_keywords[:5]]) if top_keywords else "ë°ì´í„° ë¶€ì¡±"
    
    # ì´ìŠˆ í™•ì‚° í¬ì¸íŠ¸ í…ìŠ¤íŠ¸í™”
    outbreak_text = "íŠ¹ì´í•œ ê¸‰ì¦ êµ¬ê°„ ì—†ìŒ"
    if outbreak_weeks:
        outbreak_text = f"{outbreak_weeks[0]} (ê²€ìƒ‰ëŸ‰ ê¸‰ì¦ ê°ì§€)"

    # 2. ê°•ë ¥í•´ì§„ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (6ê°€ì§€ ìš”ì†Œ ë°˜ì˜)
    prompt = f"""
    ë‹¹ì‹ ì€ ìˆ˜ì„ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ [ì¢…í•© ë¶„ì„ ë°ì´í„°]ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{query}' ë¸Œëœë“œì— ëŒ€í•œ ì‹¬ì¸µ ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

    ğŸ“Š [ì¢…í•© ë¶„ì„ ë°ì´í„°]
    1. ì´ìŠˆ í™•ì‚° í¬ì¸íŠ¸ (Outbreak): {outbreak_text}
    2. ìµœë‹¤ ì–¸ê¸‰ëŸ‰ ì¼ì (Peak Date): {most_frequent_date}
    3. ì–¸ê¸‰ëŸ‰ ì¦ê°ë¥  (Growth Rate): {mention_change_rate} (ìµœê·¼ 30ì¼ ê¸°ì¤€)
    4. ë¸Œëœë“œ ê°ì„± ë¶„ì„ (Sentiment): {sentiment_label} (ê¸ì • {positive_score}%, ë¶€ì •/ì¤‘ë¦½ {100 - positive_score}%)
    5. íŠ¸ë Œë“œ ì–¸ê¸‰ ë‹¨ì–´ (Keywords): {keywords_str}
    6. ì´ ì–¸ê¸‰ëŸ‰ (Total Volume): {total_mentions}ê±´

    ğŸ“ [ì‘ì„± ê°€ì´ë“œ]
    ìœ„ 6ê°€ì§€ ë°ì´í„°ë¥¼ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ 500ì ë‚´ì™¸ë¡œ ì‘ì„±í•˜ë˜, ë‹¤ìŒ 4ê°€ì§€ ì„¹ì…˜ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
    ë°°ê²½ ì§€ì‹ì´ë‚˜ ì™¸ë¶€ ì •ë³´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

    1. ğŸ“ˆ [ê²€ìƒ‰ëŸ‰-ì–¸ê¸‰ëŸ‰ êµì°¨ ë¶„ì„]
       - 'ì´ìŠˆ í™•ì‚° í¬ì¸íŠ¸'ì™€ 'ì–¸ê¸‰ëŸ‰ ì¦ê°ë¥ ', 'ìµœë‹¤ ì–¸ê¸‰ì¼'ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì„¸ìš”.
       - ì˜ˆ: ê²€ìƒ‰ëŸ‰ì´ ê¸‰ì¦í•˜ë©´ì„œ ì‹¤ì œ ì–¸ê¸‰ëŸ‰ë„ í­ë°œì ìœ¼ë¡œ ëŠ˜ì—ˆëŠ”ì§€, ì•„ë‹ˆë©´ ê²€ìƒ‰ë§Œ ëŠ˜ê³  ì–¸ê¸‰ì€ ì—†ëŠ”ì§€ ì§„ë‹¨.

    2. ğŸ—£ï¸ [ì—¬ë¡  ë° ê°ì„± ì§„ë‹¨]
       - ê¸ì • ë¹„ìœ¨({positive_score}%)ê³¼ '{sentiment_label}' íŒì •ì„ ê¸°ë°˜ìœ¼ë¡œ ì†Œë¹„ìì˜ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•˜ì„¸ìš”.
       - ê¸ì •ì´ ë†’ë‹¤ë©´ ë¸Œëœë“œ íŒŒì›Œë¥¼, ë‚®ë‹¤ë©´ ë¦¬ìŠ¤í¬ ìš”ì¸ì„ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì„¸ìš”.

    3. ğŸ”‘ [íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë§¥ë½ ë¶„ì„]
       - ë„ì¶œëœ ìƒìœ„ í‚¤ì›Œë“œ({keywords_str})ë“¤ì´ ì™œ ë‚˜ì™”ëŠ”ì§€, ê°ì„±/ì–¸ê¸‰ëŸ‰ ë°ì´í„°ì™€ ì—°ê²° ì§€ì–´ í•´ì„í•˜ì„¸ìš”.

    4. ğŸ’¡ [ì „ë¬¸ê°€ ì „ëµ ì œì–¸]
       - ìœ„ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ ë§ˆì¼€íŒ… ë˜ëŠ” ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì°¨ì›ì˜ êµ¬ì²´ì ì¸ í–‰ë™ ì „ëµì„ í•œ ì¤„ë¡œ ì œì•ˆí•˜ì„¸ìš”.
    """

    # 3. Gemini API í˜¸ì¶œ (ìŠ¤ë ˆë“œ ì•Œë¦¼ ê¸°ëŠ¥ í¬í•¨)
    print("\n--- ğŸ§  Gemini 2.5 AI ì‹¬ì¸µ ë¦¬í¬íŠ¸ ìš”ì²­ ì‹œì‘... ---")
    
    if api_key:
        # 5ì´ˆ ì•Œë¦¼ ìŠ¤ë ˆë“œ í•¨ìˆ˜
        def print_loading_status(stop_event):
            elapsed = 0
            while not stop_event.is_set():
                time.sleep(5)
                elapsed += 5
                if not stop_event.is_set():
                    print(f"   ... {elapsed}ì´ˆ ê²½ê³¼: AIê°€ 6ê°€ì§€ ì§€í‘œë¥¼ êµì°¨ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤ ğŸ“Š")

        stop_loading = threading.Event()
        loader_thread = threading.Thread(target=print_loading_status, args=(stop_loading,))
        loader_thread.daemon = True 

        try:
            loader_thread.start()
            
            # ëª¨ë¸ëª…: 2.5-pro (ì•ˆë˜ë©´ 1.5-pro ì‚¬ìš©)
            url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?key={api_key}"
            
            headers = {'Content-Type': 'application/json'}
            payload = {"contents": [{"parts": [{"text": prompt}]}]}
            
            response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=30)
            
            stop_loading.set()
            loader_thread.join() 
            
            if response.status_code == 200:
                result = response.json()
                try:
                    ai_text = result['candidates'][0]['content']['parts'][0]['text']
                    print("-> âœ… AI ì‹¬ì¸µ ë¦¬í¬íŠ¸ ìƒì„± ì„±ê³µ!")
                    return markdown.markdown(ai_text)
                except (KeyError, IndexError):
                    print(f"-> âš ï¸ ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: {result}")
            else:
                print(f"-> âš ï¸ AI ìš”ì²­ ì‹¤íŒ¨ (ìƒíƒœ ì½”ë“œ: {response.status_code})")
                if response.status_code == 404:
                     print("-> íŒíŠ¸: 'gemini-2.5-pro' ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. URLì„ 'gemini-1.5-pro'ë¡œ ë³€ê²½í•´ ë³´ì„¸ìš”.")
                
        except Exception as e:
            stop_loading.set()
            print(f"-> âš ï¸ AI ì—°ê²° ì˜¤ë¥˜: {e}")
    else:
        print("-> âš ï¸ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # 4. [ì•ˆì „ì¥ì¹˜] ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ ë¦¬í¬íŠ¸ (Fallback)
    print("-> ğŸ”„ ê·œì¹™ ê¸°ë°˜ ë¦¬í¬íŠ¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
    
    fallback_report = f"ğŸ“ˆ [êµì°¨ ë¶„ì„]: '{query}'ì˜ ì–¸ê¸‰ëŸ‰ì€ ì´ {total_mentions}ê±´ì´ë©°, ìµœê·¼ ì¦ê°ë¥ ì€ {mention_change_rate}ì…ë‹ˆë‹¤. ìµœë‹¤ ì–¸ê¸‰ì¼ì€ {most_frequent_date}ì…ë‹ˆë‹¤.\n\n"
    fallback_report += f"ğŸ—£ï¸ [ì—¬ë¡ ]: ê¸ì • ë¹„ìœ¨ {positive_score}%ë¡œ '{sentiment_label}' ì„±í–¥ì„ ë³´ì…ë‹ˆë‹¤.\n\n"
    fallback_report += f"ğŸ”‘ [í‚¤ì›Œë“œ]: ì£¼ìš” íŠ¸ë Œë“œ ë‹¨ì–´ëŠ” '{keywords_str}' ì…ë‹ˆë‹¤.\n\n"
    fallback_report += "ğŸ’¡ [ì œì–¸]: ìƒì„¸ ë°ì´í„° í™•ì¸ í›„ ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½ì´ í•„ìš”í•©ë‹ˆë‹¤."
    
    return markdown.markdown(fallback_report)

# ----------------------------------------------------
# --- í•µì‹¬ ë¶„ì„ í•¨ìˆ˜ (ì§€í‘œ ê³„ì‚° ë° ê°ì„± ë¶„ì„) ---
# ----------------------------------------------------

def find_outbreak_weeks(trend_df, change_threshold=0.5):
    """ ì›”ê°„ ê²€ìƒ‰ëŸ‰ ë¹„ìœ¨ì„ ê¸°ì¤€ìœ¼ë¡œ ì „ì›” ëŒ€ë¹„ ê²€ìƒ‰ëŸ‰ì´ ê¸‰ì¦í•œ ì›”ê°„ì„ ì°¾ìŠµë‹ˆë‹¤. """
    print(f"\n--- 6ë‹¨ê³„ ë¶„ì„: ì´ìŠˆ í™•ì‚° ì›”ê°„ ì¶”ì¶œ ì‹œì‘ (ì „ì›” ëŒ€ë¹„ {change_threshold * 100:.0f}% ì´ˆê³¼ ì¦ê°€ ê¸°ì¤€) ---")
    
    if trend_df.empty or 'date' not in trend_df.columns:
        print("-> âŒ ê²€ìƒ‰ëŸ‰(ë°ì´í„°ë©) ë°ì´í„°ê°€ ì—†ì–´ í™•ì‚° ì›”ê°„ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    now = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0).replace(day=1)
    filtered_trend_df = trend_df[trend_df['date'] < now].copy()
    filtered_trend_df = filtered_trend_df.sort_values(by='date')
    
    if filtered_trend_df.empty:
        print("-> âŒ ìœ íš¨í•œ ê³¼ê±° ì›”ê°„ ë°ì´í„°ê°€ ì—†ì–´ í™•ì‚° ì›”ê°„ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    filtered_trend_df['ratio'] = pd.to_numeric(filtered_trend_df['ratio'], errors='coerce')
    filtered_trend_df.dropna(subset=['ratio'], inplace=True)
    filtered_trend_df['prev_ratio'] = filtered_trend_df['ratio'].shift(1).fillna(0)
    
    filtered_trend_df['change_rate'] = filtered_trend_df.apply(
        lambda row: (row['ratio'] - row['prev_ratio']) / row['prev_ratio'] 
                     if row['prev_ratio'] > 0 else (100.0 if row['ratio'] > 0 else 0), 
        axis=1
    )
    
    outbreak_months_df = filtered_trend_df[
        ((filtered_trend_df['prev_ratio'] > 0) & (filtered_trend_df['change_rate'] > change_threshold)) | 
        ((filtered_trend_df['prev_ratio'] == 0) & (filtered_trend_df['ratio'] > 0))
    ].copy()
    
    outbreak_results = []
    if not outbreak_months_df.empty:
        outbreak_months_df = outbreak_months_df.sort_values(by='ratio', ascending=False)
        for _, row in outbreak_months_df.iterrows():
            date_obj = row['date']
            year = date_obj.year
            month = date_obj.month
            
            # ì›”ì˜ ì²« ë‚ ì„ ê¸°ì¤€ìœ¼ë¡œ ì£¼ì°¨ ê³„ì‚° (ISO ì£¼ì°¨ ì‚¬ìš©)
            # ISO ì£¼ì°¨: ì›”ìš”ì¼ì„ ì£¼ì˜ ì‹œì‘ìœ¼ë¡œ í•¨
            iso_calendar = date_obj.isocalendar()
            week_of_year = iso_calendar[1]  # ISO ì£¼ì°¨ (1~53)
            
            # ë” ì§ê´€ì ì¸ ì›”ë³„ ì£¼ì°¨ ê³„ì‚°: ì›”ì˜ ì²« ë‚ ë¶€í„°ì˜ ì£¼ì°¨
            # (0-based indexì—ì„œ +1 í•˜ì—¬ 1~5 ë²”ìœ„)
            first_day_of_month = date_obj.replace(day=1)
            days_since_month_start = (date_obj - first_day_of_month).days
            week_of_month = (days_since_month_start // 7) + 1
            
            current_ratio = row['ratio']
            prev_ratio = row['prev_ratio']
            
            rate_str = f"{row['change_rate'] * 100:.1f}% ì¦ê°€" if row['prev_ratio'] > 0 else "ì‹ ê·œ ë°œìƒ (ì „ì›” 0)"
            
            # ë…„/ì›”/ì£¼ì°¨ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
            date_with_week = f"{year}-{month:02d}/{week_of_month}ì£¼ì°¨"
            outbreak_results.append(f"{date_with_week} (í˜„ì¬ ë¹„ìœ¨: {current_ratio:.1f}, ì „ì›”: {prev_ratio:.1f}, {rate_str})")
            
        print(f"-> âœ… ì´ {len(outbreak_results)}ê°œì˜ ê²€ìƒ‰ëŸ‰ ê¸‰ì¦ ì›”ê°„ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
    return outbreak_results

def calculate_lexicon_score(text_corpus):
    """ ê°ì„± ì‚¬ì „ì„ ê¸°ë°˜ìœ¼ë¡œ ê¸ì •ì„± ì§€ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. """
    print("\n--- 7ë‹¨ê³„ ë¶„ì„: ìˆœìˆ˜ Python ê°ì„± ì‚¬ì „ì„ ì´ìš©í•œ ê°ì„± ë¶„ì„ ì‹œì‘ ---")
    
    cleaned_text = re.sub('[^ê°€-í£\s]', '', text_corpus).lower()
    pos_count = sum(len(re.findall(r'\b' + re.escape(word) + r'\b', cleaned_text)) for word in POSITIVE_WORDS)
    neg_count = sum(len(re.findall(r'\b' + re.escape(word) + r'\b', cleaned_text)) for word in NEGATIVE_WORDS)

    total_sentiment_count = pos_count + neg_count

    if total_sentiment_count == 0:
        print("-> ê°ì„± ë‹¨ì–´ê°€ í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ë˜ì§€ ì•Šì•„ ì¤‘ë¦½ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
        return 50.0

    positive_score = (pos_count / total_sentiment_count) * 100
    
    print(f"-> ê¸ì • ë‹¨ì–´: {pos_count}íšŒ, ë¶€ì • ë‹¨ì–´: {neg_count}íšŒ")
    print(f"-> ê³„ì‚°ëœ ê¸ì •ì„± ì§€ìˆ˜: {positive_score:.2f}%")
    
    return positive_score

def classify_sentiment(positive_score):
    """ ì‚¬ìš©ì ì§€ì • ê·œì¹™ì— ë”°ë¼ ìµœì¢… ê°ì„±ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤. """
    if positive_score > 60:
        return f"ğŸŸ¢ ê¸ì • ({positive_score:.1f}%)"
    else: 
        return f"ğŸ”´ ë¶€ì • ({positive_score:.1f}%)"

def calculate_key_metrics(df):
    """ ìµœë‹¤ ì–¸ê¸‰ëŸ‰ ë‚ ì§œì™€ ì–¸ê¸‰ëŸ‰ ì¦ê°ë¥ (ìµœê·¼ 30ì¼ vs ì´ì „ 30ì¼)ì„ ê³„ì‚°í•©ë‹ˆë‹¤. """
    default_date = 'N/A'
    default_rate = 'N/A'
    
    temp_df = df.copy()
    print("\n--- 8ë‹¨ê³„ ë¶„ì„: ìµœë‹¤ ì–¸ê¸‰ ë‚ ì§œ ë° ì¦ê°ë¥  ê³„ì‚° ì‹œì‘ ---")
    
    try:
        temp_df['postdate'] = pd.to_datetime(temp_df['postdate'], errors='coerce').dt.normalize()
        temp_df.dropna(subset=['postdate'], inplace=True)
    except Exception as e:
        print(f"-> âŒ ì–¸ê¸‰ëŸ‰ ë¶„ì„: ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨. {e}")
        return default_date, default_rate

    if temp_df.empty: return default_date, default_rate

    # 1. ìµœë‹¤ ì–¸ê¸‰ëŸ‰ ë‚ ì§œ
    daily_counts = temp_df['postdate'].dt.normalize().value_counts()
    most_frequent_date = default_date
    if not daily_counts.empty:
        max_count = daily_counts.max()
        most_frequent_date = daily_counts[daily_counts == max_count].index.max().strftime('%Y-%m-%d')
        print(f"-> ìµœë‹¤ ì–¸ê¸‰ëŸ‰ ë‚ ì§œ: {most_frequent_date} ({max_count}íšŒ)")

    # 2. ì–¸ê¸‰ëŸ‰ ì¦ê°ë¥ 
    now = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    start_d1, end_d1 = now - timedelta(days=30), now 
    start_d2, end_d2 = now - timedelta(days=60), now - timedelta(days=30)

    mentions_d1 = temp_df[(temp_df['postdate'].dt.normalize() >= start_d1) & (temp_df['postdate'].dt.normalize() < end_d1)].shape[0]
    mentions_d2 = temp_df[(temp_df['postdate'].dt.normalize() >= start_d2) & (temp_df['postdate'].dt.normalize() < end_d2)].shape[0]

    mention_change_rate = default_rate

    if mentions_d2 > 0:
        change_rate = ((mentions_d1 - mentions_d2) / mentions_d2) * 100
        mention_change_rate = f"{change_rate:+.1f}%" 
    elif mentions_d1 > 0:
        mention_change_rate = "+100% (ì‹ ê·œ)"
    else:
        mention_change_rate = "0.0%"

    print(f"-> ì¦ê°ë¥  ë¶„ì„: ìµœê·¼ 30ì¼({mentions_d1}ê±´) vs ì´ì „ 30ì¼({mentions_d2}ê±´). ì¦ê°ë¥ : {mention_change_rate}")
    return most_frequent_date, mention_change_rate


# ----------------------------------------------------
# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (Flask API ì—”ë“œí¬ì¸íŠ¸ì— ì‚¬ìš©) ---
# ----------------------------------------------------

def run_full_analysis(search_query: str, competitor_query: str, client_id: str, client_secret: str, max_results: int, static_folder: str) -> dict:
    """
    ë¸Œëœë“œ í‰íŒ ë¶„ì„ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ JSON ì‘ë‹µìš© ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    
    analysis_results = {
        "query": search_query,
        "competitor_query": competitor_query,
        "status": "FAILURE",
        "message": "ë¶„ì„ ì‹¤íŒ¨: ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜",
        "key_metrics": {},
        "sentiment_analysis": {},
        "trend_analysis": {},
        "visualization_urls": {}
    }

    # 1. ìœ íš¨ì„± ê²€ì¦
    if not is_valid_query(search_query):
        analysis_results["message"] = "ìì‚¬ ê²€ìƒ‰ì–´ ìœ íš¨ì„± ê²€ì¦ì— ì‹¤íŒ¨í•˜ì—¬ ë¶„ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."
        return analysis_results
    
    # ë¸Œëœë“œëª… ê²€ì¦ (ê²½ìŸì‚¬ ë¶„ì„ì¼ ë•ŒëŠ” ìŠ¤í‚µ)
    if not competitor_query:
        if not is_brand_name(search_query, client_id, client_secret):
            analysis_results["message"] = "ë¸Œëœë“œëª… ê²€ì¦ì— ì‹¤íŒ¨í•˜ì—¬ ë¶„ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."
            return analysis_results
    
    is_comp_valid = competitor_query and is_valid_query(competitor_query)

    print("\n==================================================")
    print("âœ… ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("==================================================")
    
    # 2. ìì‚¬ ë°ì´í„° ìˆ˜ì§‘ ë° í†µí•©
    blog_df = fetch_naver_search_results(search_query, 'blog', client_id, client_secret, max_results)
    news_df = fetch_naver_search_results(search_query, 'news', client_id, client_secret, max_results)
    total_df = pd.concat([blog_df, news_df], ignore_index=True)
    
    # 3. ê²€ìƒ‰ëŸ‰ ì¶”ì´ ë¶„ì„ (ì›”ê°„)
    trend_df = get_search_trend(search_query, client_id, client_secret)
    
    # 4. ê²½ìŸì‚¬ ë°ì´í„° ìˆ˜ì§‘
    competitor_df = pd.DataFrame()
    if is_comp_valid and competitor_query:
        comp_blog_df = fetch_naver_search_results(competitor_query, 'blog', client_id, client_secret, max_results)
        comp_news_df = fetch_naver_search_results(competitor_query, 'news', client_id, client_secret, max_results)
        competitor_df = pd.concat([comp_blog_df, comp_news_df], ignore_index=True)

    if total_df.empty:
        analysis_results["message"] = "ìˆ˜ì§‘ëœ ìì‚¬ ë¸”ë¡œê·¸/ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ì–´ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤."
        analysis_results["status"] = "INSUFFICIENT_DATA"
        return analysis_results

    # 5. ë¶„ì„ ë° ì§€í‘œ ê³„ì‚°
    total_description = ' '.join(total_df['description'].astype(str).tolist())
    positive_score = calculate_lexicon_score(total_description) 
    final_sentiment = classify_sentiment(positive_score)
    most_frequent_date_result, mention_change_rate_result = calculate_key_metrics(total_df)
    initial_outbreak_months = find_outbreak_weeks(trend_df, change_threshold=0.5) 
    
    # 6. ì‹œê°í™” ë° URL ì €ì¥
    gc.collect() 
    
    urls = {}
    
    # 6-1. ê°ì„± ì›Œë“œí´ë¼ìš°ë“œ
    pos_plot, neg_plot, all_plot, top7_keywords_df = visualize_sentiment_word_clouds(
        total_df, POSITIVE_WORDS, NEGATIVE_WORDS
    )
    
    urls["positive_wordcloud"] = save_and_get_url(
        lambda: pos_plot, "sentiment_positive_wc.png", static_folder
    )
    urls["negative_wordcloud"] = save_and_get_url(
        lambda: neg_plot, "sentiment_negative_wc.png", static_folder
    )
    urls["combined_wordcloud"] = save_and_get_url(
        lambda: all_plot, "sentiment_combined_wc.png", static_folder
    )
    
    # 6-2. ì›”ë³„ ì–¸ê¸‰ëŸ‰ ì¶”ì´
    urls["monthly_frequency"] = save_and_get_url(
        lambda: visualize_post_frequency(total_df, frequency_type='monthly'),
        "mention_monthly_freq.png", static_folder
    )
    
    # 6-3. ì£¼ë³„ ì–¸ê¸‰ëŸ‰ ì¶”ì´ (ìµœê·¼ 6ê°œì›”)
    urls["weekly_frequency"] = save_and_get_url(
        lambda: visualize_post_frequency(total_df, frequency_type='weekly'),
        "mention_weekly_freq.png", static_folder
    )
    
    # 6-4. ê²€ìƒ‰ëŸ‰ vs ì–¸ê¸‰ëŸ‰ í†µí•©
    urls["combined_trend"] = save_and_get_url(
        lambda: visualize_combined_trend(total_df, trend_df),
        "mention_vs_search_trend.png", static_folder
    )
    
    # 6-5. ê²½ìŸì‚¬ ë¹„êµ (ê²½ìŸì‚¬ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ)
    if is_comp_valid and not competitor_df.empty:
        urls["competitor_comparison"] = save_and_get_url(
            lambda: visualize_competitor_mention_comparison(search_query, total_df, competitor_query, competitor_df),
            "competitor_comparison.png", static_folder
        )
    else:
        urls["competitor_comparison"] = None
    
    # 7. ìµœì¢… ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
    analysis_results = {}

    # ê¸°ë³¸ ì •ë³´
    analysis_results["query"] = search_query
    analysis_results["competitor_query"] = competitor_query
    analysis_results["status"] = "SUCCESS"
    analysis_results["message"] = f"'{search_query}' ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."

    # í•µì‹¬ ì§€í‘œ
    analysis_results["total_mentions"] = len(total_df)
    analysis_results["most_frequent_date"] = most_frequent_date_result
    analysis_results["mention_change_rate"] = mention_change_rate_result
    analysis_results["competitor_mentions"] = len(competitor_df) if competitor_query else 0

    # ê°ì„± ë¶„ì„
    analysis_results["final_sentiment_label"] = final_sentiment 
    analysis_results["positive_percentage"] = int(float(f"{positive_score:.2f}")) 
    analysis_results["top_keywords"] = top7_keywords_df.to_dict('records') if not top7_keywords_df.empty else []

    # íŠ¸ë Œë“œ ë¶„ì„
    analysis_results["outbreak_weeks"] = initial_outbreak_months
    analysis_results["trend_data_available"] = not trend_df.empty

    # ì‹œê°í™” URL
    analysis_results["visualization_urls"] = urls

    # ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸
    analysis_results["post_list"] = total_df[[
        'title', 
        'postdate', 
        'channel_name', 
        'link'
    ]].rename(columns={
        'postdate': 'date', 
        'channel_name': 'author'
    }).to_dict('records')

    # âš ï¸ [ì„¤ì •] ì—¬ê¸°ì— Gemini API í‚¤ë¥¼ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš” (ë”°ì˜´í‘œ ì•ˆì—)
    MY_GEMINI_KEY = "dd"

    # AI ë¦¬í¬íŠ¸ ìƒì„± í˜¸ì¶œ
    analysis_results["ai_report"] = generate_smart_report(
        query=search_query,
        total_mentions=len(total_df),
        sentiment_label=final_sentiment,
        positive_score=int(float(f"{positive_score:.2f}")),
        top_keywords=top7_keywords_df.to_dict('records') if not top7_keywords_df.empty else [],
        outbreak_weeks=initial_outbreak_months,
        trend_available=not trend_df.empty,
        most_frequent_date=most_frequent_date_result,
        mention_change_rate=mention_change_rate_result,
        
        api_key=MY_GEMINI_KEY
    )

    print("\n==================================================")
    print("âœ… ìµœì¢… ë¶„ì„ ê²°ê³¼ JSON ìƒì„± ì™„ë£Œ. Flask ì‘ë‹µ ì¤€ë¹„.")
    print("==================================================")

    return analysis_results

# ----------------------------------------------------
# --- ì½˜ì†” í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰ ë¸”ë¡ ---
# ----------------------------------------------------

if __name__ == "__main__":
    print("ì´ ì½”ë“œëŠ” Flask ì•±ì˜ ë°±ì—”ë“œ ëª¨ë“ˆë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ì‹¤ì œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œëŠ” Flask í™˜ê²½ê³¼ API í‚¤, í°íŠ¸ ê²½ë¡œ ë“±ì´ í•„ìš”í•©ë‹ˆë‹¤.")
